<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://pa-legg.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://pa-legg.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-16T10:31:53+00:00</updated><id>https://pa-legg.github.io/feed.xml</id><title type="html">Prof. Phil Legg</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Security Data Analytics – A Pipeline Process</title><link href="https://pa-legg.github.io/blog/2023/security-data-analytics-a-pipeline-process/" rel="alternate" type="text/html" title="Security Data Analytics – A Pipeline Process"/><published>2023-09-11T21:58:36+00:00</published><updated>2023-09-11T21:58:36+00:00</updated><id>https://pa-legg.github.io/blog/2023/security-data-analytics--a-pipeline-process</id><content type="html" xml:base="https://pa-legg.github.io/blog/2023/security-data-analytics-a-pipeline-process/"><![CDATA[<p>What would a typical pipeline or workflow look like in a cyber security data analytics investigation? We can think of this as a 5-stage process:</p> <ul><li><strong>Collect: </strong>What data do we need to collect, and how will we collect it? This is fundamental as it requires us to carefully think about the problem that we hope to address. Typically, we require a *sensor* in order to collect data — much like if we want to collect data about the world around us we need to use some sensor or measurement instrument to do this effectively. Sensors could be software-based, such as a Python script that continual checks and records every new login on a particular server, or that logs all the current running processes on a workstation. IoT devices may enable us to sense something about the physical world (for example, temperature) and transmit this data to a centralised server. Humans are also powerful sensors, often overlooked where thinking about cyber security. For example, humans may sense observations about the workplace such as employee engagement or disgruntlement, that if captured effectively could help mitigate potential threats.</li><li><strong>Store: </strong>How will we store this data? Is it practical to store the volume of data required? Is there are specific format that will make this data more useable for our further analysis and investigation? Common file data formats such as Comma Separated Values (CSV) and Javascript Object Notation (JSON) are popular amongst the data science communities. For larger distributed data collection, databases may be a preferable approach, that may be better suited to manage multiple users either reading or writing data simulateneously. Databases are widely used in complex systems, and typically either hold structured or unstructured data. Sequel (SQL) is a traditional database language used for structured data, whilst NoSQL unstructured methods such as MongoDB have been adopted more recently, that effectively store a collection of JSON documents. Whichever approach is taken, you will need to consider how often is data being written into your data store, how often it is being read from your data store, and how many users will need to do this at any one time, to manage the scalability of how your data is stored.</li><li><strong>Combine / Transform:</strong> Once we have a way of storing our data, what do we want to do with it? Typically, we may be looking to gather data over time to observe whether there is some change in behaviour, or we may be looking to combine multiple data sources to help explain why a particular observation has occurred, such as a cyber security breach or some suspicious activity on our network. We may also want to transform the data in some manner to make it easier to explore and understand. <a href="https://en.wikipedia.org/wiki/Aggregate_function">Aggregation</a> is a widely used process for transforming data. To aggregate data simply means to reduce this data to some form of summary; a common example would be to take the mean value of the data collected over a given time period. For example, given a number of raw packets captured from a network, we may want to know the total number of packets observed every minute, or the average size of the packets received each minute. This provides a much clearer indication when comparing activity over time, rather than studying individual packets. This can also be described as <a href="https://en.wikipedia.org/wiki/Feature_extraction">feature extraction</a> since we are deriving a set of features that characterise our raw data, which could then be used to inform a machine learning model. It may also be described as <a href="https://hackernoon.com/what-steps-should-one-take-while-doing-data-preprocessing-502c993e1caa">data pre-processing</a>, since we are essentially preparing our data so that it is in a more appropriate format to work with for further investigation. Other examples may include rescaling absolute values, such as rescaling RGB pixel values in an image or rescaling RAM usage to be a percentage of the total RAM available in a system.</li><li><strong>Analyse:</strong> We now have both the raw data and a feature-based representation of the data. We may sometimes refer to this as having <em>focus and context</em> of our data, where we can both drill down into the detail of each data point whilst observing something about the overall data distribution. With this in mind, we now need to establish what kind of analyse we need for our task. We may wish to examine data based on some thresholding techniques — for example, identify all cases where more than <em>X</em> network connections have been established during a 1-hour period. We may also wish to examine data using [<a href="https://ieeexplore.ieee.org/document/8716381">signature-based techniques</a> — for example, identify all software executables where the MD5 hash matches against a known set of examples (often described as a dictionary, and widely used for anti-virus detection). There are other more sophisticated forms of analysis we may also want to consider. We may want to perform <a href="https://towardsdatascience.com/machine-learning-classifiers-a5cc4e1b0623">classification</a> of our data, where we have a set of possible outcomes or groups, and we want to identify which group each instance of our data belongs to (for example, classification of different malware family samples). Another form of analysis that we may wish to perform is <a href="https://machinelearningmastery.com/findings-comparing-classical-and-machine-learning-methods-for-time-series-forecasting/">forecasting</a>. For example, if we have observed 2 data breaches in the last 12-month period, can we forecast whether we will fall victim again, based on possible mitigations that could be deployed. <a href="https://towardsdatascience.com/unsupervised-machine-learning-clustering-analysis-d40f2b34ae7e">Clustering</a> is another form that is widely used because it allows us to group similar observations together, to then examine whether they are similar under some set assumptions. This sounds similar to classification, however in classification we have labelled samples to train on, whereas in clustering we do not have any labels. We will discuss this further in Chapter 4. Finally, there is outlier and anomaly detection — a key part of security analysis. Given some observation, can we identify when something is different, and more specifically, under what conditions is it anomalous. We will describe this in further detail in due course.</li><li><strong>Visualise: </strong>Following our analysis, we need to plan how to communicate our findings. We may use 2-dimensional charts and plots to do this, such as line plots, bar charts, scatter plots, as well as other forms of visual plots. It’s important to appreciate that numerical values in a table also communicate data — sometimes a table of numerical values will perform more effectively than a visualisation, often if the data is small. As the data becomes larger however, visualisation helps to summarise this and convey details clearer. Depending on the task, we may find that 3-dimensional visualisation is appropriate. This could be a 3-dimensional plot, or it could be a 3-dimensional reconstruction of some scene, depending on the application. Importantly, we need to think about how the end-user will receive information from the chosen representation. Focus-and-context is a common technique to allow users to examine some aspect in detail, whilst also showing how this fits within the broader data. More and more nowadays, interactive techniques in visualisation need to be considered, for how a user will learn and understand the data through interaction, such as parameter adjustment, or zoom and filter of the data.</li></ul> <h4>Data Science Workflows</h4> <p>Whilst we have described a possible workflow above, it’s important to recognise that there is no single right way of doing data science. More specifically, if we keep in mind that we are doing analysis for a purpose, to help inform a story, whether that be exploratory or explanatory, we need to establish the question that we are asking, so that we can strive to answer this.</p> <p>Here we see another possible workflow, where we decide on a question, and we acquire data to support this question. We clean and transform this data, and we perform our data analysis — much like we have discussed already. However, it is important to recognise that this is not a single one-time process, nor we will necessary ask the right question to being with. In this example, we see three “loops” back to earlier stages of the process — we may need to update our analysis, for example, if we decide that we need to consider more or different features about the data. We may examine our output and present our results, to find that we need more data — for example, if we are examining the presence of an insider threat, do we have sufficient information about the actions they have taken, or do we only have coverage of a subset (e.g., we may have network activity, but we have no record of file access activity). Finally, we may decide that we need to update our question, for example, is our original question not achievable, or do we need to be more specific with how the question is formed?</p> <p>Two further models of analysis are presented here — both showing how analysis is very much an iterative process that will continually evolve. (Further detail in the book by <a href="https://www.vismaster.eu/wp-content/uploads/2010/11/VisMaster-book-lowres.pdf">D. Keim “Mastering the Information Age: Solving Problems with Visual Analytics, 2010</a>). In the literature, often researchers will describe this as having a “human-in-the-loop” or by adopting a “visual analytics loop”. This is more and more common nowadays as dashboards and interactive analysis techniques have become a normal means of practice.</p> <h4>Introducing Python Notebooks</h4> <p>Notebooks are used for rapid prototyping of ideas, to test a theory or to derive some experiment. In particular, when we talk of notebooks we are describing computational notebooks, where we want to jot down ideas, and test these, whilst being able to jot down code samples and examine our results and findings quickly. Scripting languages such as Python and Javascript make it much easier to rapidly construct a piece of code to process some data, rather than having to compile code to source, like you would do in C or Java. Notebook environments provide a clean approach for integrating code, text, graphics, and other forms of media, using a web browser to interact with this rather than relying on traditional IDEs or text editors. Notebooks stem back to software tools such as Mathematica, however, the most commonly used Notebook format today is the Jupyter Notebook (formerly called the iPython notebook — hence the file extension ipynb). Jupyter is an amalgomation of Julia, Python, and R — the three programming languages it was originally designed for — however over the years a number of extensions have been developed to support many more languages, making it a defacto standard for computational notebooks. Notebooks essentially store their data in JSON format, and the Jupyter environment then renders this correctly for user interaction. More recently, Jupyter Lab has been proposed as a browser-based environment for working with notebooks that supports a number of modern features compared to the original Jupyter environment.</p> <p>Python has grown in popularity significantly over the last 10 years. There are a number of reasons, such as the ability to use the REPL command line (Read-Evaluate-Print-Loop), which links well with our view of iterative practice for data science. Furthermore, the wealth of libraries developed for Python mean that many tasks can be performed quickly using these libraries rather than coding from scratch. In particular, for Data Science, there are a number of libraries that are seen as the standard: NumPy (Numerical Python), SciPy (Scientific Python), Matplotlib (a Python graphing library originating from MATLAB conventions), Pandas (Powerful Data Analysis), as well as scikit-learn (used for machine learning), Tensorflow, Keras and PyTorch (used for deep learning), and NLTK (Natural Language Toolkit).</p> <p>As part of this broader course, we will focus on a “Hello Security Data Analytics” challenge, to demonstrate how we can use a data science workflow with the Jupyter notebook environment to solve a cyber security challenge. The manager of the security operations centre (SOC) suggests that her analysts are becoming inundated with “trivial” alerts ever since a new data set of indicators was introduced into the Security Information and Event Management (SIEM) system. As a cyber security data scientist, they have asked for your help to reduce the number of “trivial” alerts without sacrificing visibility of security alerts. This is a good problem to tackle through data analysis, and we should be able to form a solid, practical question to ask after we perform some exploratory data analysis and hopefully arrive at an answer that can help the SOC. In the next session, we will look at how we can use Jupyter notebooks to help us solve this problem.</p> <h4>Further reading</h4> <p>JupyterLab Documentation</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=9be6356f0a8d" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">Security Data Analytics - An Introduction</title><link href="https://pa-legg.github.io/blog/2023/security-data-analytics-an-introduction/" rel="alternate" type="text/html" title="Security Data Analytics - An Introduction"/><published>2023-09-11T20:30:26+00:00</published><updated>2023-09-11T20:30:26+00:00</updated><id>https://pa-legg.github.io/blog/2023/security-data-analytics---an-introduction</id><content type="html" xml:base="https://pa-legg.github.io/blog/2023/security-data-analytics-an-introduction/"><![CDATA[<h3>Security Data Analytics – An Introduction</h3> <p>Data analytics is increasingly used as a first line of defence in cyber security – and yet many organisations are still to embrace this – partly due to a lack of resourcing and partly due to a lack of knowledge for what kind of security analysis they may even require. Fundamentally, we all live in a highly digital world, where we interact online, or we interact with other physical systems that are connected online, and where all these interactions result in more data being collected – from networking traffic to system and user logs – our computers generate massive volumes of data every minute. Now consider the different forms of digital equipment that are also connected and generating data – from laptops to smartphones, servers to light bulbs, payment card systems to CCTV, and anything else that may be Internet-enabled – we now have a wealth of data logs capturing a whole host of activities from these devices, that enable our technology-driven world.</p> <p>So, what happens when these devices do not perform as expected? What if our laptop suffers a malware attack, our CCTV is taken offline by a Denial of Service attack, or an employee downloads confidential records and copies these to a removable drive? Security data analytics is concerned with how we protect our systems and information from potential threats, through enhanced understanding of our systems and the data that underpins these.</p> <h4>What about Visualisation?</h4> <p>Analysing data is one thing – communicating this analysis to make well-informed decisions is another all together. Visualisation techniques enable us to make greater understanding of complex data, and ultimately allow us to make better decisions about our data.</p> <ul><li><strong>Exploratory visualisation</strong> is where visualisation methods may be used to explore a dataset to uncover some unknown traits or characteristics. It may not even be known what is being sought after initially, however through filtering and selection, the analyst may then discover new details within the data.</li><li><strong>Explanatory visualisation</strong> is used to communicate these findings to somebody else. A particular chart or graph may be used that emphasises some characteristic of the data so that this is clearly visible for others and so that it provides contextual meaning to explain the situation.</li></ul> <p>We can think about exploratory visualisation as supporting our analysis of data, and explanatory visualisation as supporting our presentation of data.</p> <blockquote><em>“The greatest value of a picture is when it forces us to notice what we never expected to see”</em> John W. Tukey, 1977</blockquote> <h4>What kind of data are we talking about?</h4> <p>Just as the security domain is broad, so to are the forms of data that may inform security. At the core is context – what are we trying to find out, and what data may help us to answer this question clearly? Furthermore, how do we then respond or act as a result of this information?</p> <p>We can think of data as the lowest level of a broader pyramid, where data informs information, information informs knowledge, and knowledge informs wisdom. Data in isolation may not be meaningful, however combined with other data, may help to build up towards these higher levels. This is often referred to as the <a href="https://en.wikipedia.org/wiki/DIKW_pyramid">DIKW pyramid</a>.</p> <h4><strong>So, what do we mean by cyber security data analytics?</strong></h4> <p>For an organisation to be secure, a clear understanding of the operational environment is required. This is often described as <a href="https://en.wikipedia.org/wiki/Situation_awareness">situational awareness</a>, which is the perception of environmental elements and events, the comprehension of their meaning, and the projection of their future states. Increasingly, organisations are deploying <strong>security operations centres (SOC)</strong>, where analysts will seek to identify suspicious behaviour and understand the context and relevance to the organisational mission, often using <strong>Security Information and Event Management (SIEM) </strong>systems.</p> <p>Technology underpins modern organisations and having insight into the business operational environment is crucial to protect it. As a first stage, ensuring the safe and correct operation of our computer systems, and our networking infrastructure is a good place to start. Network traffic data (e.g., packet captures) can help to indicate what data has been communicated over a network, and what actions have been carried out as a result of this (e.g., access to a particular URL, or downloading of large files). Intrusion Detection Systems (IDS) are commonly used to inspect networking inbound and outbound network traffic, to identify suspicious activities. IDSs will generate log files, and these logs constitute another informative data attribute. Similarly, firewall rules can help understand how the network is configured, and Intrusion Prevention Systems (IPS) will make decisions and act on IDS activity to prevent potential harm.</p> <p>The remit of cyber security is far and wide and goes beyond traditional computers and network security. A holistic view is required of what we want to protect, and what attack vectors may be used to gain access. Therefore, aspects such as physical security, people security, and process security also need to be understood. Physical security may require CCTV, IoT sensor monitoring or GPS tracking. People security may require text analytics of social media and email usage. Process security may require analysis of business process models, supply chain security, organisational hierarchy information, and operational practice. Technology continues to influence how we conduct business across the global, and therefore we need to ensure that we understand our threat landscape and have clear monitoring in place to understand potential harms. In many cases, we are interested in spatial-temporal data, i.e., in what location did the activity occur and at what time? Given our highly connected society, location is becoming increasingly challenging (are we looking at the location of the attacker, the location of the data, the location of the breached system?), and as for time, devices are logging activities faster than we can humanly inspect them. There is then the need for big data cyber security analytics – to make this flow of data manageable and insightful, to highlight key attributes in the data, and to enable informed decisions to be made to respond and react to potential threats.</p> <blockquote><strong>Security is about understanding systems, the people, and the processes that act upon these systems, such that they remain secure.</strong></blockquote> <p>Can we ever be fully secure? Probably not, but with greater insight of observed activities, we can manage this more effectively. Data analytics and visualisation techniques are one step towards achieving this.</p> <h4>Data-driven Storytelling</h4> <p>We discussed earlier the idea that data visualisation can be used for exploratory and explanatory uses. In the latter, we may want to tell a story about our data, often described as Data-Driven Storytelling. So what makes for a good data story? There are five aspects that may be pertinent to the story.</p> <ul><li><strong>Trend</strong>: We may want to observe the historical or seasonal pattern of observations within our data, such as when do most people log on during the day, or what the monthly variation of demand on our servers is.</li><li><strong>Novelty</strong>: We are looking to find something new within our data observation, such as a new trend in our data.</li><li><strong>Outlier</strong>: We are looking to find where something occurs differently within an existing trend in our data.</li><li><strong>Forecasting</strong>: We want to determine what the future forecast of our data may be based on our historical observations.</li><li><strong>Debunking</strong>: We want to determine how our data supports or disproves a possible theory of what is happening.</li></ul> <p>Typically, in security, we are looking for novelty and outliers, based on the historical trend, to then provide a forecast of what may be if we do not intervene. Where we can couple observations against either “known-bad” activities (e.g., malware attacks), or show that observations are clearly deviating from “known-good” activities (e.g., insider threat), we can provide some insight into the underlying activity to determine whether action is required.</p> <h4><strong>AI and Cyber Security</strong></h4> <p>DarkTrace, CheckPoint, Symantec, Sophos, FireEye, Cynet, Fortinet, Vectra, and Cylance. These are just a handful of vendors that now use Artificial Intelligence and Machine Learning as a part of their products and services for cyber security defence – there are plenty others too, but this just gives you an impression of the direction that the industry is moving in.</p> <p>Cyber security requires a holistic view to identify what should be protected, and how may it be vulnerable to attack. The volume of data generated by today’s systems means that humans cannot analyse this raw data effectively. With AI and machine learning techniques, we can filter and manage data observations, whilst visualisation can help human analysts understand and communicate about observations and appropriate responsive actions.</p> <h4><strong>Further reading and resources</strong></h4> <p>Sarker, I.H., Kayes, A.S.M., Badsha, S. et al. <a href="https://journalofbigdata.springeropen.com/articles/10.1186/s40537-020-00318-5">Cybersecurity data science: an overview from machine learning perspective</a>. J Big Data 7, 41 (2020). https://doi.org/10.1186/s40537-020-00318-5</p> <p>Maayan, G. <a href="https://datasciencedojo.com/blog/data-science-cybersecurity">Cybersecurity revolutionized with rich data science</a>. Datasciencedojo (2020).</p> <p><a href="https://www.youtube.com/watch?v=x1B9WrRPtOc">Big Data Security Analytics — Key Findings of the Study — YouTube</a></p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=2e9b524c772b" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry></feed>