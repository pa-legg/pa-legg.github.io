<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://pa-legg.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://pa-legg.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-02T22:23:29+00:00</updated><id>https://pa-legg.github.io/feed.xml</id><title type="html">Prof. Phil Legg</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">AI for rapid prototyping in education</title><link href="https://pa-legg.github.io/blog/2025/ai-for-rapid-prototyping-in-education/" rel="alternate" type="text/html" title="AI for rapid prototyping in education"/><published>2025-06-27T14:51:47+00:00</published><updated>2025-06-27T14:51:47+00:00</updated><id>https://pa-legg.github.io/blog/2025/ai-for-rapid-prototyping-in-education</id><content type="html" xml:base="https://pa-legg.github.io/blog/2025/ai-for-rapid-prototyping-in-education/"><![CDATA[<p>It seems that there are many educators and academics grappling with the idea of what modern AI tools mean for education, when a student may use AI to solve the problem or complete the assignment, and then everyone complains that the student has ‘cheated’…</p> <p>We need to move away from this way of thinking…</p> <p>AI techniques, such as those that have become synonymous like ChatGPT and Gemini, have the ability to help educators just like they are being used in many other disciplines. I use GPTs for coding and rapid prototyping of concepts and ideas that I want to test. At this point, it may then sound like it is merely for these ‘coders’ to dabble with, but with a few straightforward ideas any educators can harness this potential.</p> <blockquote>Here’s a simple prompt: <em>“Generate the code for a simple interactive webpage for demonstrating the concept of encryption for students.”</em></blockquote> <p>Sure enough, our trusted partner says “OK”, and then produces a simple HTML-based web application for teaching some basic encryption principles.</p> <p><em>&lt;!DOCTYPE html&gt;<br/>&lt;html lang=”en”&gt;<br/>&lt;head&gt;<br/> &lt;meta charset=”UTF-8&quot; /&gt;<br/> &lt;meta name=”viewport” content=”width=device-width, initial-scale=1.0&quot;/&gt;<br/> &lt;title&gt;Encryption Demo&lt;/title&gt;<br/> &lt;style&gt;<br/> body {<br/> font-family: Arial, sans-serif;<br/> max-width: 600px;<br/> margin: 2em auto;<br/> padding: 1em; …</em></p> <p>We can then take this code and save it as a .html file, or we could even embed this code directly within our VLE (Virtual Learning Environment). The screenshot and link below shows what this output gives:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*SLjxg9oIz1uKSInCU8x6qA.png"/><figcaption>Interactive encryption demo generated in HTML using ChatGPT</figcaption></figure> <p><a href="https://pa-legg.github.io/resources/ai-web-examples/encryption-examples/encryption-example01.html">Example01</a></p> <p>This is a great little example to start with — we can perform caesar cipher with a given rotation parameter, and we can see the output of this in an interactive manner. Not only does GPT provide this, but it will also suggest ways that the code can be enhanced further — e.g., different decryption algorithms, and real-time encryption/decryption. We can simply say “Yes” to these additional ideas, and GPT will then run along making further iterations of our simple concept. The examples below show the further iterations.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*O5rk1Jpz-_pJ3W7CVUbMww.png"/><figcaption>Expanding our example further to include decryption stages</figcaption></figure> <p><a href="https://pa-legg.github.io/resources/ai-web-examples/encryption-examples/encryption-example02.html">Encryption Example 02</a></p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*AgpAIGgWmUUXuP5cRw1JhA.png"/><figcaption>Expanding futher still with multiple encryption schemes available</figcaption></figure> <p><a href="https://pa-legg.github.io/resources/ai-web-examples/encryption-examples/encryption-example03.html">Encryption Example 03</a></p> <p>I’ve used this for a few different subject areas, with different interactive components all generated automatically. I’ll be doing a few short follow ups to showcase these also.</p> <p>The key factor however, is that I can now use AI to create new, bespoke learning environments for my students. If I can do this and save time in the production, I can have students use the examples, and then use the class to elaborate further on the topic area, potentially with further interactive examples. You could even generate further interactive examples in partnership with your students! Talk about co-creation!! :)</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=32fe4a89bea8" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">Security Data Analytics — 09 — Future Research Directions</title><link href="https://pa-legg.github.io/blog/2025/security-data-analytics09future-research-directions/" rel="alternate" type="text/html" title="Security Data Analytics — 09 — Future Research Directions"/><published>2025-06-22T10:58:42+00:00</published><updated>2025-06-22T10:58:42+00:00</updated><id>https://pa-legg.github.io/blog/2025/security-data-analytics09future-research-directions</id><content type="html" xml:base="https://pa-legg.github.io/blog/2025/security-data-analytics09future-research-directions/"><![CDATA[<h3>Security Data Analytics — Session 09 — Future Research Directions</h3> <p><strong><em>*** This material is session 9 from the previous Security Data Analytics and Visualisation course that I led at UWE until 2024. It is now shared for reference purposes. ***</em></strong></p> <p>We have covered a lot of ground through this course, from the initial ideas around cyber security analytics, through to managing workflows, machine learning and visualisation, and different applications of analysis that may be required. Of course, future research will continue to evolve and we will see greater uses of data analytics to understand the world around us and how best to secure this.</p> <h3>Visualisation for Cyber Security</h3> <p>The IEEE VizSec conference started in 2004 as the Workshop on Visualization and Data Mining for Computer Security, as a co-located event as part of the IEEE Vis conference — one of the largest International conferences on the topic of data visualisation. The conference is still an integral part of IEEE Vis, and continues to attract high-quality research publications. You can find more details about VizSec at <a href="https://vizsec.org/">https://vizsec.org/</a>. There is also an online proceedings browser available at <a href="https://vizsec.dbvis.de/">https://vizsec.dbvis.de/</a>.</p> <h3>Future of Data Analytics in Cyber Security</h3> <p>Here are just a small sample of articles and further reading that relate to the use of Machine Learning and Data Analytics for Cyber Security, ranging from Connected Autonomous Vechicles, HealthTech, and Industrial IoT — 3 key areas that are seeing significant impact from ML and data analytics, and that will plan an important role within society in our future.</p> <ul><li><a href="https://billatnapier.medium.com/introduction-to-machine-learning-and-splunk-1ef256add6b1">Introduction to Machine Learning and Splunk (Prof Bill Buchanan)</a></li><li><a href="https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/661135/cyber-security-connected-automated-vehicles-key-principles.pdf">The Key Principles of Cyber Security for Connected Autonomous Vehicles</a></li><li><a href="https://arxiv.org/abs/2007.08041">A Survey on Security Attacks and Defense Techniques for Connected and Autonomous Vehicles</a></li><li><a href="https://www.bbc.co.uk/news/technology-55411830">Health to be on cyber-security’s front line in 2021</a></li><li><a href="https://www.sciencedirect.com/science/article/pii/S0378512218301658?casa_token=OvW__y7jq_4AAAAA:haGdYeEEo664ZN4XmMcSwBJY2RWjac3Ge22n0oP5PrxETA-1_b33_0B8eNQRARlA1KnmSKrSlsY">Cybersecurity in healthcare: A narrative review of trends, threats and ways forward</a></li><li><a href="https://www.nccoe.nist.gov/projects/use-cases/energy-sector/iiot">Securing the Industrial Internet of Things</a></li><li><a href="https://iiot-world.com/ics-security/cybersecurity/four-most-hard-to-solve-iiot-security-issues/">Four most hard to solve IIoT security issues</a></li></ul> <h3>Yet more resources</h3> <ul><li><a href="https://infosecjupyterthon.com/introduction.html">Infosec Jupyterthon</a>: A 2 day online workshop for all things Jupyter and how this can be used for InfoSec. With many contributors including speakers from Microsoft, this is a fantastic resource.</li><li><a href="https://mybinder.org/">MyBinder</a>: Turns a github repo into an interactive notebook environment for code reproducability.</li><li><a href="https://www.github.com/">GitHub</a>: Online code hosting repositories — over 11 million Jupyter notebooks hosted on GitHub currently.</li><li><a href="https://github.com/OTRF">Open Threat Research Forge</a></li><li><a href="https://github.com/OTRF/bloodhound-notebooks">Bloodhound Notebooks</a>: Notebooks created to attack and secure Active Directory environments.</li><li><a href="https://github.com/OTRF/Security-Datasets">Security Datasets</a>: The Security Datasets project is an open-source initiatve that contributes malicious and benign datasets, from different platforms, to the infosec community to expedite data analysis and threat research.</li><li><a href="https://github.com/OTRF/ThreatHunter-Playbook">ThreatHunter-Playbook</a></li><li><a href="https://www.unb.ca/cic/datasets/">Canadian Institute for Cybersecurity — Datasets</a>: An excellent data repository with related academic papers</li></ul> <h3>Further Reading</h3> <ul><li><a href="https://academic.oup.com/cybersecurity/article/1/1/93/2366512">Peter Hall, Claude Heath, Lizzie Coles-Kemp, Critical visualization: a case for rethinking how we visualize risk and security, Journal of Cybersecurity, Volume 1, Issue 1, September 2015, Pages 93–108, https://doi.org/10.1093/cybsec/tyv004</a></li><li><a href="https://informationsecurity.uibk.ac.at/pdfs/WB2020_sok_cyberrisk_snp.pdf">Daniel W. Woods and Rainer Bohme. Systematization of Knowledge: Quantifying Cyber Risk</a></li><li><a href="https://link.springer.com/article/10.1007/s12243-021-00889-1">Aouedi, O., Piamrat, K., Hamma, S. et al. Network traffic analysis using machine learning: an unsupervised approach to understand and slice your network. Ann. Telecommun. (2021). https://doi.org/10.1007/s12243-021-00889-1</a></li><li><a href="https://ieeexplore.ieee.org/document/9086268">M. A. Ayub, W. A. Johnson, D. A. Talbert and A. Siraj, “Model Evasion Attack on Intrusion Detection Systems using Adversarial Machine Learning,” 2020 54th Annual Conference on Information Sciences and Systems (CISS), 2020, pp. 1–6, doi: 10.1109/CISS48834.2020.1570617116.</a></li><li><a href="https://www.unb.ca/cic/datasets/ids-2017.html">Canadian Research Institute for Cybersecurity Datasets</a></li><li><a href="https://www.sciencedirect.com/science/article/pii/S016740481930118X">Markus Ring, Sarah Wunderlich, Deniz Scheuring, Dieter Landes, Andreas Hotho. A survey of network-based intrusion detection data sets</a></li></ul> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=4b6811d7c2c0" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">Security Data Analytics — 08 — File, Image and Video Analytics</title><link href="https://pa-legg.github.io/blog/2025/security-data-analytics08file-image-and-video-analytics/" rel="alternate" type="text/html" title="Security Data Analytics — 08 — File, Image and Video Analytics"/><published>2025-06-22T10:58:18+00:00</published><updated>2025-06-22T10:58:18+00:00</updated><id>https://pa-legg.github.io/blog/2025/security-data-analytics08file-image-and-video-analytics</id><content type="html" xml:base="https://pa-legg.github.io/blog/2025/security-data-analytics08file-image-and-video-analytics/"><![CDATA[<h3>Security Data Analytics — Session 08 — File, Image and Video Analytics</h3> <p><strong><em>*** This material is session 8 from the previous Security Data Analytics and Visualisation course that I led at UWE until 2024. It is now shared for reference purposes. ***</em></strong></p> <h3>Visualising Files</h3> <p>Often, we may want to understand the data that resides on a computer, such as what files exist and their content. Furthermore, there exist a wider range of file formats that we may want to examine — ranging from binary data and executable files, through to text files and office documents, and even multimedia content such as image and video formats. In this session, we will begin to examine methods of analysis and visualisation that can help to understand such content beyond typical manual investigation.</p> <h3>Binary File Visualisation</h3> <p>All data files stored on a computer system, no matter what their content is, can be expressed as a byte stream. This is the raw data that is processed by the computer to convert this data into a meaningful representation for the end-user, or to inform some other process or executable. As defined on Wikipedia, “the byte is a unit of digital information that most commonly consists of eight bits. Historically, the byte was the number of bits used to encode a single character of text in a computer and for this reason it is the smallest addressable unit of memory in many computer architectures.” An 8-bit representation can be used to express values in the range 0–255 (i.e., 00000000 to 11111111 in binary notation). Working with a byte stream for a file, we can then visualise these numerical values. Given the scale of information represented within a single file, we need a compact visual representation as we may be attempting to plot many values. Pixel visualisation is an effective technique where we can map numerical values to pixel colour values, to create an image based on the underlying numerical data.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/512/0*r9Ypfr-RdL5Sgnuc.png"/></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/512/0*qoipMwZ1nxApve4Z.png"/></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/512/0*cz7I95o6rorayshy.png"/></figure> <p>In this example, we simply map each byte of the file to a colour value running left-to-right, top-to-bottom (i.e., run-length). This preserves order in the file content. The visualisation size is determined by the size of the file, which may be useful, however can make comparative analysis difficult. In the above, we show 3 files (a docx, a docx with password protection, and a docx with AES encryption), and we use a fixed width of 512 bytes. We can begin to examine similarities and differences between these files using the visualisation scheme. Most notable is that the password example shows the additional password data, and some commonalities with the original file, whereas in the AES example the data is fully encrypted and not recoverable without the encryption key.</p> <p>Another approach is similar to that of ‘n-grams’ in text analytics, where we consider pairs of bytes to form a ‘digram visualisation’. In this setting, we take each pair of bytes and treat them as the X and Y coordinates to plot a point on a scatter plot. Points can be scaled based on number of occurrences if necessary, or colour-coded based on byte position or sequence. Below we can see examples of ‘jpeg’ file, a ‘txt’ file and a ‘docx’ file.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/492/0*o_A8cU_obXoun528.png"/></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/488/0*L485CiiXEhX9D7JK.png"/></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/502/0*Sl4dRF3ELOuVKZOT.png"/></figure> <p>In the examples, the scatter plot is bound between the values of 0–255, and so helps maintain a consistent representation to help comparative analysis (e.g., frequency of byte pairs, density of byte pairs). A tri-gram could be created using triplets of bytes and plotted using a 3-D scatterplot (see examples at <a href="https://codisec.com/binary-visualization-explained/">https://codisec.com/binary-visualization-explained/</a>).</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/658/0*5zZFuq5dBPxzPVEx.png"/></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/655/0*ue7MVIvE2aOJb9mI.png"/></figure> <h3>Image and Video Analysis</h3> <p>Whilst binary file visualisation can be utilised for all forms of digital data, images and videos are a unique data format that lend themselves to further analysis. As visual stimuli, there are inherently understood and interpreted by humans. Image and video files are particularly large compared to many other file formats that are commonly used on most computer systems. Furthermore, image and video data has inherent spatial information that is fundmental for understanding their content. We will explore this further in this section.</p> <p>Firstly, why may we be interested in image and video analysis for security? Closed circuit television (CCTV) has been used for many years to monitor physical environments from a security perspective so that incidents can either be identified at the time of the event, or at the very least, be examined after the incident has occurred. This results in many hours of video footage, that is likely difficult to search and retrieve specific content from. Beyond searching video based on time, how may we be able to search video based on other characteristics, such as when a person wearing a blue jacket is identified? We may also want to identify more sophisticated concepts, such as when particular actions occur (activity recognition), when particular persons are in the scene (facial recognition), or when a change in behaviour is observed (anomaly detection). In many ways, what we are interested in is a way of summarising video content, where a video is denoted as a series of sequential images of which we need to filter the volume of data being examined.</p> <p>To motivate this example further, let us first recap on video attributes. A video is essentially a stack of sequential images. A single colour image is expressed by a computer as a 3-dimensional matrix: width x height x channels (where channels is typically RGB — red, green and blue). A video can therefore be expressed as a 4-dimensional matrix: width x height x channels x frames. For our example, we have a short video of 2 minutes 32 seconds. At the standard 30 frames a second, we have 4574 frames that make up the video. Our video has a frame height of 480 pixels and a frame width of 360. In short, our short video has over 2 billion data points.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/536/0*bbHAGUwVgdP4eqrq.png"/></figure> <p>What may we want to achieve from our analysis? Firstly, we may want to identify changes in our video stream, for example, if we have a CCTV stream, can we identify key frames where something may have changed? Secondly, can we group (cluster) similar frames together? This may help us to group similar patterns of activity, and identify activities that stand out as different in some way. For our examples, we will work with the McGill Real-World Face Video Dataset which is available online (and via Blackboard). It consists of 20 unique users where each users is filmed for a short interview clip of a couple of seconds. We will aim to identify changes in the scene (i.e., when a new users appears in the video) and also cluster frames for each user together.</p> <p>As our first stages of analysis, we need to make the data more manageable to work with. We can begin by extracting a single frame per second (rather than 30 frames per second). Since we are not trying to track motion between frames this is fine for our application, and more importantly, makes the problem much more manageable at 153 frames rather than 4574. Secondly, we can reduce each video frame to a greyscale image, meaning that we have only a single colour channel rather than RGB. Again, colour is not required for the purpose of our application so we can afford to remove this. We now have a set of 153 greyscale images that represent our video, where each image is 480x360 resolution (172,800 pixels). We are going to adopt a technique from earlier in the course of principal component analysis (PCA). Recall that PCA will take high dimensional data and reduce this to a low dimensionality (e.g., 2 or 3-dimensional). Each point in our projection should represent a single image, and so the high-dimensional data is essentially the set of pixels that make up an image. Whilst spatial information in images is important for us humans to understand image content, it matters little to a machine. Therefore, we could reshape our image matrix, so that instead of being rectangular, it is a single row of data, or to put it another way, it is a vector of size (1, 172800). Providing we remap each frame in the same manner, then each column in our new matrix represents a unique pixel position that can be compared across all frames of our video. Having remapped each image frame to be a vector, we then stack these in sequence to form a new matrix of size (153, 172800).</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/496/0*nTDUAW6ssCwhEqR5.png"/></figure> <p>Having remapped our data, we can now simply perform PCA on our matrix — as discussed before this will essentially identify the features (columns) of greatest variance across all instances (i.e., image frames) of our data. Luckily, we can use the sci-kit learn library to perform this task quickly and easily. We can plot the resulting values as a scatter plot, where we can begin to identify points that have clustered together. We now have a set of unlabelled points, and so as we have seen earlier in the course, we can use k-means clustering to assign labels to each point. We know in advance that k should be 20 — we have 20 unique users in the video data — and so having performed k-means clustering we can colour-code the scatter plot according to the group assignment. That’s it — we have now taken a video and identified the similarity between extracted image frames to cluster similar frames together.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/679/0*F74q3iKshyn7higm.png"/></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/684/0*GkW8ILmBAQoovtXb.png"/></figure> <p>To check this, we could display all images within a given group to see whether the frames do in fact appear similar — here we can see that these 12 frames all show the same woman. It should be noted that this method may not give perfect results — for example, removal of RGB may loss too much information — but hopefully it demonstrates the concept.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/679/0*RPiaUCqIz8TXc4Wb.png"/></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/680/0*BohlUP4WiR4Lfn5e.png"/></figure> <p>As for recognising change between video frames, we can easily use the same matrix remapping to achieve this. For each row, we calculate the absolute difference in pixel values between this row and the next. We can then summarize the total pixel differences using the mean to express this as a single difference value between two frames. Using a line plot as shown here, we can observe peaks that are likely the scene changes in the video, and so we can identify an appropriate threshold for scene change detection either manually using the plot, or automatically by separating the peaks from the smaller frame differences (which are a result of both motion and image noise / video compression artefacts).</p> <h3>Summary</h3> <p>We have discussed methods for visualising file content, be it as raw binary data in the form of a byte stream, or in the case of richer multimedia such as image and video, how we can process raw image pixel data. One of the key points to raise is that we have been able to adopt many of the methods we have used previously in the course. This is important to recognise, since file data is essentially just another form of data that we can work with. Image and video present some further interesting challenges, primarily due to the inherent understanding that humans have of images and videos, however we can perform analysis on pixel data much as we can of any other data attributes.</p> <p>Multimedia content such as image and video is shared online in such volumes nowadays that we need better methods for analysis of such data. As discussed, videos and inherent to humans but mean little to a machine — so how would a machine work to prevent the sharing of indecent video content? Given the nature of re-posting and sharing of videos, this becomes important to stop the spread of malicious or dangerous content online. Examples have ranged from videos of terrorist attacks, unwanted sharing of sexual videos, and intentional graphic violence injected in children’s video. Given how online video is now considered general behaviour for many households, ensuring video content is safe for users is a vital area of protecting cyber space. Video analysis also naturally plays into insider threat detection, open source intelligence, and forensic investigations. As technology continues to evolve with connected autonomous vehicles, health tech, and industrial IoT, video streaming and video analysis is a crucial aspect of such systems to analyse, detect and respond to real-time conditions — be it reacting to a moving vehicle to avoid a collision, or adjusting a medical dosage based on image analysis of a patient. Cyber criminals wanting to compromise such systems will inevitably exploit image analysis techniques — a prime example being the generating of deep fakes to falsify video content that is shared remotely. Therefore, as cyber security professionals, it is important to understand and acknowledge these attack vectors, and begin to consider how best to defend against them. As adversarial learning proves, relying on artificial intelligence alone may not help to combat the situation, but understanding how to process image and video content, and how to contextual video content to understand the story being told will ensure that as defenders we are better prepared to protect our systems, especially those that are becoming more and more embedded within our society.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=ca74bf915312" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">Security Data Analytics — 07 — Text Analytics</title><link href="https://pa-legg.github.io/blog/2025/security-data-analytics07text-analytics/" rel="alternate" type="text/html" title="Security Data Analytics — 07 — Text Analytics"/><published>2025-06-22T10:57:39+00:00</published><updated>2025-06-22T10:57:39+00:00</updated><id>https://pa-legg.github.io/blog/2025/security-data-analytics07text-analytics</id><content type="html" xml:base="https://pa-legg.github.io/blog/2025/security-data-analytics07text-analytics/"><![CDATA[<h3>Security Data Analytics — Session 07 — Text Analytics</h3> <p><strong><em>*** This material is session 7 from the previous Security Data Analytics and Visualisation course that I led at UWE until 2024. It is now shared for reference purposes. ***</em></strong></p> <p>As a primary form of communication, the written (or spoken) word provides a massive wealth of information for many applications. For the most part we have been dealing with numerical data so far. Yet, there are plenty of applications where we would want to examine text rather than numerical data — for example, email analysis, social media analysis, product reviews, legalese, document classification, and many more. Natural Language Processing (NLP) is a subject area in it’s own right — but for now, we will introduce the topic and explore some of the methods that could be used to for security analysis.</p> <p>What kind of “processing” may we want to achieve? We may want natural language generation (e.g., creating a chat bot that can respond to user questions), topic modelling (e.g., classification of document types), sentiment analysis (e.g., understanding emotional traits), text clustering (e.g., relationship between words), named entity recognition (e.g., what are the ‘things’ mentioned in a sentence — such as the blue car drove down the road). We may also want to identify key words of interest from a large text corpus, we can use a method known as term frequency-inverse document frequency which we will discuss shortly.</p> <p>How does text analytics relate to security needs? We may want to examine user emotion through their use of language — common in psychological assessment such as insider threat detection. We may want to classify conversations or news articles — especially given the wealth of text information online, and how rapidly new content appears through social media sites and the like. We may also want to authenticate users based on their language, or observe when language may change — this could be in part of identifying if information is genuine, or if an attempt is made to falsify information from a given source. The rise of “fake news” on social media makes this an important security issue related to how we trust and scrutinise online materials. As alluded to earlier, humans are excellent at understanding language, however computers are not. Therefore, we need to establish models that operate in similar ways to how we understand and interpret language — recognising that these will be limited in their performance but that they are much more scalable than human resource — leaving humans to then verify the outcomes of the models.</p> <h3>Word Occurrences</h3> <p>A simple approach to begin with would be to examine the occurrence (or count) of each unique word within a document. This example takes the Computer Security wikipedia article, and assesses the number of times each word appears on the page to develop a topic model. As we may expect, “security” is the most used word in the article — however we can learn some other useful information from this, such as “cyber”, “information” “network”, and “systems” also occurring frequently. This essentially allows us to build a dictionary of related terms that could be used to connect topics together. This concept is often referred to as a ‘bag of words’ model.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/479/0*bpWghmINhb748pvh.png"/></figure> <p>We can use this same approach to assess how frequently a given word (or set of words) occur within some text (or a set of documents). Whilst this sounds fairly simple, it is very effective. It is essentially how password brute forcing works — take the rockyou dataset of 14 million passwords, and scan a set of webpages to see if these dictionary terms occur, and you could potentially identify websites that are based on weak credentials. This opens up a range of possibilities in the scope of open-source intelligence (OSINT) — how to gather and analyse information that is openly available online. Other uses may explore dictionaries such as the Linguistic Inquiry Word Count (LIWC) that is popular in psychology and can be used to identify positive and negative sentiment, along with other linguistic features.</p> <p>Another simple yet powerful extension to search for word occurrences, is searching for word pairs (or triples, etc.). We call these <strong>n-grams</strong>, where n is the number of words occurring together. Here, we now start to uncover much more context about the words, given that we can see what typically comes before or after the words of interest.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/471/0*SAIse4wNyDw3OBdV.png"/></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/481/0*pObwlmu8ndnrfKbZ.png"/></figure> <h3>Term Frequency — Inverse Document Frequency</h3> <p><a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">TF-IDF</a> is a powerful technique to identify word of interest based on their occurrence, whilst also not being hindered by commonly-occurring words that may not necessarily be of interest. Suppose we have a set of documents. For a single document, we may be interested in how often a particular word is mentioned. However, if this word is truly of importance to that specific document, then the expectation is that it should not be so frequent across other documents. For example, if we had documents reporting common vulnerabilities (e.g., CVEs), then a specific vulnerability such as Heartbleed may be highly mentioned in its respective document, but not mentioned in any others. This would mean that it should score higher than a word such as “cyber”, which whilst it may occur more frequently in the same document, may also occur quite frequently across all documents.</p> <p>TF-IDF is calculated as follows:</p> <ul><li>TF(t) = number of times t appears in a document / total number of terms in document</li><li>IDF(t) = log (total number of documents / number of documents with t in)</li></ul> <p>Let’s consider our example using the words “Heartbleed” and “Cyber”. Suppose “Heartbleed” occurs 10 times within a 100 word document: TF(Heartbleed) = 10/100 = 0.1. Suppose we have 1000 documents and “Heartbleed” occurs in 10 of these: IDF(Heartbleed) = log(1000 / 10) = 2. Then, TF-IDF = TF * IDF = 0.1 * 2 = 0.2. Suppose now we look at the word “Cyber”, and assume it occurs 30 times within the 100 word document: TF(Cyber) = 30/100 = 0.3, but it also occurs in 750 of the 1000 documents: IDF(Cyber) = log(1000 / 750) = 0.124. Then, TF-IDF = TF * IDF = 0.3 * 0.124 = 0.0372. Here, Heartbleed is scored higher than Cyber because in relation to the overall document set it is deemed of greater significance.</p> <h3>Recommender Systems</h3> <p>How can a system learn to make recommendations? Recommender systems become popular through their use in market basket analysis (e.g. Tesco Clubcard) for predicting shopping habits and recommending products that would likely result in a purchase, however they can be used in other applications to identify groupings of similarity and relations between items (e.g., words).</p> <p>Suppose we have 10 popular items, and we have a record of 10 users as to whether they would buy that item or not (denoted by either a 1 or a 0). We can use this information to identify items that are purchased together, or customers who are similar in their purchasing habits. If we have a new customer, we can initiate a “cold” profile simply by taking the average of all existing users. If we learn that this new user then likes chicken, we can update our recommendations by filtering, so that we average all users who also like chicken, to obtain a revised prediction for this user. In this example, if Kyle was a new customer there would be 90% probability of him buying pasta (since 9 out of 10 of existing customers do already) and 50% probability of him buying a pear (since 5 out of 10 existing customers do already). If we then observe that Kyle buys beef (i.e., he adds it to his basket), we can filter our dataset based on customers who also buy beef. This would then indicate a 100% probability of him also buying pasta (all customers who have bought beef have also bought pasta), and a 75% probability of buying a pear (since 3 of 4 customers who bought beef also bought a pear). Likewise, the probability of Kyle buying an apple goes from 40% to 0% because of him buying beef. There is a practical notebook available where you can explore this concept further.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/548/0*rkeVMr4kQzbz369s.png"/></figure> <p>Now instead of this example being about items in a shopping basket, what if this was about words in a news article? We could learn to characterise topics such as ‘sports’, ‘politics’, ‘entertainment’, and ‘travel’, and we could identify articles that appear similar to each other — essentially allowing us to cluster data much like we have done earlier in this course. In a similar manner, we could also recommend the next word in a passage of text — much like how predictive text services function. Text models therefore can be used to both predict what will appear next, and also as a means of validate what has appeared next, and whether this conforms to the expected model of behaviour.</p> <h3>Spam Detection</h3> <p>Another popular task for text analytics is identifying whether an email is spam or not. As the use of email has exploded, so has the volume of spam email received — however many providers have developed good models for detecting between genuine email and spam. How do they work? Many systems use an approach known as Naïve Bayes, based on Bayes’ Theorem: P(spam | X) = P(X | spam) * P(spam) / P(X).</p> <p>This equation denotes conditional probabilities, and can essentially be described as the following:</p> <ul><li>P(spam | X): What is the probability of this email being spam, given that it contains the word X?</li><li>P(X | spam): What is the probability of the word X occurring, given that this email is spam?</li><li>P(spam): What is the probability of this email being spam?</li><li>P(X): What is the probability of the word X occurring?</li></ul> <p>Even with incomplete data, we can begin to populate values for this equation based on observations we have seen in our data (i.e., in the email set we already have). Over time, our probabilities can be updated so that our spam detection improves as new data is observed (essentially by clicking the ‘junk’ option to inform the system of undetected cases). This is a powerful technique using Bayes’ — we can not possibly have a model for all types of spam email as there are infinite permutations of spam — however conditional probability allows us to estimate this efficiently. <a href="https://hackernoon.com/how-to-build-a-simple-spam-detecting-machine-learning-classifier-4471fe6b816e">An example of using Naïve Bayes for spam detection is available here</a>.</p> <h3>More on Text Analytics</h3> <p>As mentioned at the beginning of this section, there is a wealth of ongoing research and development in the area of text analytics that is increasingly useful for examining online materials and informing decisions about data (e.g., social media posts, fake news, etc.). <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">Recurrent Neural Networks</a> (RNN) are a special neural net architecture that work particularly well on text as they are designed for sequential data. We can consider text to be sequential as there is a specific order to words to make up a structured and meaningful sentence (e.g., “The cat sat on the…” — a RNN would likely predict the next word to be “mat”). In particular, <a href="https://en.wikipedia.org/wiki/Long_short-term_memory">Long Short-Term Memory</a> (LSTM) networks are form of RNN that are widely used, which overcome some of the early limitations in RNNs for how historical information is maintained over time. The main distinction between these methods compared to traditional neural networks, is that RNN and LSTM take a sequence as an input (e.g., a set of features with some inherent order, such as a time-series), rather than a single observation of data (e.g., a set of features, or a single image)</p> <p>We have shown methods that rely on feature representation, such as “one-hot encoding” of words to numerical vectors — essentially converting a list of words to a vector of zero, where a one then represents the specific word. Whilst this can work in some cases, it can be constrained when a large word list is required (e.g., the complete English language). Other language models exist, for example, <a href="https://en.wikipedia.org/wiki/Word2vec">Word2Vec</a> has become adopted as a popular method for word embeddings. This essentially provides a model that can convert a given word to a vector representation in a more compact representation (e.g., an embedding) compared to “one-hot encoding”. This is similar in concept to PCA — it provides a dimensionality reduction on the data to present a meaningful yet compact representation. Word2Vec can be used to define a ‘continuous bag of words’, where given a set of words, what one word would fit within the set. Similarly, it can also be used to define ‘skip-grams’, where given a single word, what set of words would fit with this? We can see that these two methods are the inverse of each other. Extensions to the Word2Vec model have been proposed such as <a href="https://medium.com/wisio/a-gentle-introduction-to-doc2vec-db3e8c0cce5e">Doc2Vec</a>, which generates vectors to summary complete documents based on the Word2Vec model, offering a model compact representation for larger document analysis task.</p> <p>In recent years there has been a great interest in language models, and developing automated models for tasks such as chatbots and image captioning. If we consider a single word as a vector, a “simple” model could be considered as a vec2vec model (e.g., word translation). In our earlier example, we may have a sequence of words that then predict a single output word (e.g. predictive text), which we could consider as seq2vec. By extension, we then may also be interested in models that adopt vec2seq (a single input mapping to a sequential output) and <a href="https://en.wikipedia.org/wiki/Seq2seq">seq2seq</a> (a sequential input mapping to a sequential output). <a href="https://keras.io/examples/nlp/lstm_seq2seq/">Language translation</a> could be considered as seq2seq since we may have a variable length input and a variable length output. Another good example would be for a conversational agent (e.g., a chatbot), since the length of the input (i.e., number of words in the question) may vary in length each time, and likewise the length of the output (i.e., the answer given by the system) may also be a variable length. Text analytics and language models are perhaps the most groundbreaking area of research in delivering artificial intelligence. <a href="https://en.wikipedia.org/wiki/GPT-3">Generative Pre-trained Transformer 3</a> (GPT-3) was released in 2020 by OpenAI in their research paper <a href="https://arxiv.org/abs/2005.14165">“Language Models are Few-Shot Learners”</a>. It uses 175 billion parameters in their learning model, but achieves near-human accuracy. Whilst it can do traditional text tasks like sentence completion, they demonstrate it’s effectiveness for truly understanding text, such as executing commands as described by a human — from building applications based on a written description, smart assistants that can recognise tasks and provide recommendation, and many other examples that are available online. There are two videos in particular that describe just some of the possible applications by <a href="https://www.youtube.com/watch?v=_x9AwxfjxvE">2 minute papers</a> and by <a href="https://www.youtube.com/watch?v=8psgEDhT1MM">Half Ideas</a>. Since then, we have seen the likes of ChatGPT and GPT-4 take to the mainstream, with incredible growth in model size, contextual understanding, and practical application. With the Internet serving to inform a model, and compute power ever-increasing, there is a great wealth of future research opportunity to be explored.</p> <h3>Further reading</h3> <ul><li><a href="https://onlinelibrary.wiley.com/doi/full/10.1002/spy2.9">Ahmed H., Traore I. and Saad S. Detecting opinion spams and fake news using text classification, Security and Privacy, 2018; 1:e9. https://doi.org/10.1001/spy2.9</a></li><li><a href="https://research.google/pubs/pub46201/">Vaswani A., Shazeer N., Parmar N., Uszkoreit J., Jones L., Gomez A., Kaiser L. and Polosukhin I. Attention is All You Need, NIPS, 2017; https://research.google/pubs/pub46201/</a></li></ul> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=72620bdd0028" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">Security Data Analytics — 06 — Visual Analytics</title><link href="https://pa-legg.github.io/blog/2025/security-data-analytics06visual-analytics/" rel="alternate" type="text/html" title="Security Data Analytics — 06 — Visual Analytics"/><published>2025-06-22T10:57:02+00:00</published><updated>2025-06-22T10:57:02+00:00</updated><id>https://pa-legg.github.io/blog/2025/security-data-analytics06visual-analytics</id><content type="html" xml:base="https://pa-legg.github.io/blog/2025/security-data-analytics06visual-analytics/"><![CDATA[<h3>Security Data Analytics — Session 06 — Visual Analytics</h3> <p><strong><em>*** This material is session 6 from the previous Security Data Analytics and Visualisation course that I led at UWE until 2024. It is now shared for reference purposes. ***</em></strong></p> <h3>What do we mean by “Visual Analytics”?</h3> <ul><li>“Visual Analytics is the science of analytical reasoning supported by a highly interactive visual interface.” [Wong and Thomas 2004]</li><li>“Visual Analytics combines automated analysis techniques with interactive visualisations for an effective understanding, reasoning and decision making on the basis of very large and complex datasets” [Keim 2010]</li><li>“Detect the expected and discover the unexpected”</li></ul> <p>We have already discussed statistics, machine learning, and visualization. Visual analytics is essentially the combination of all three — the use of interactive visual interfaces to support statistical and analytical reasoning about data. A key emphasis here is the interactivity of the system — much like the knowledge generation feedback loop and the data science workflows discussed in Section 2, visual analytics is about how we develop analytical reasoning through iterative use of the system to interact, filter, and zoom in on key details of the data, as part of the exploration process. We have discussed static forms of data visualisation in Section 5, however the greatest challenge of any visualization is carefully deciding on what data should be shown, and how this should be represented. Visual analytics essentially allow us to interactively update the parameters of the visualization to create a dynamic and engaging interactive experience that helps inform decision making.</p> <p>If we consider automated methods, and we consider visualization methods, what are their strengths and weaknesses? Automated methods scale well to large data, however they may suffer from local optima problems, or run in a “black box” fashion that does not facilitate understanding of their process. Visualisation methods can be interactive, but can suffer from scalability. Visual analytics aim to combine the best of both approaches, where a user will alternate between visual and automated methods, and provides a collaborative approach for problem-solving and story-telling between the user and the machine.</p> <p>Let us consider the ability matrix here. We know that computers excel in some tasks, yet humans excel in others. Visual analytics aims to find the appropriate balance between the two. Often, people describe it as “providing insight” — yet, we need to consider “what do we mean by insight?”. Insight is something that is generated by a human, not a computer. So then how does a computer system (e.g., a visual analytics tools) help to generate insight? <a href="https://arxiv.org/pdf/1305.5670.pdf">Arguably, visualization and visual analytics are about “saving time” in a manner that supports cognition</a>.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/630/0*4Nwqzpk6hbONuHBh.png"/></figure> <h3>Dashboards</h3> <p>Dashboards are now widely used in many analysis platforms. The term originates from the car dashboard — we do not need to know every detail about the underlying system, such as the exact operations of cylinders and other mechanisms under the bonnet, however we do need the key details such as current speed, and we need a means to be alerted when things go wrong. The same is true in many cyber security scenarios — what we want to know is whether the infrastructure is operating as intended, some key details about the operation of the infrastructure, and if there is a problem, will the dashboard alert us correctly and in a timely-manner, with useful and meaningful information such that we can diagnose the problem and return to normal operations. A simple Google search for “visual analytics dashboard” will reveal a number of different designs and structures that have been used. Key summary statistics shown using pie charts and radial plots are often quite common — used in a number of “business” dashboards for presenting company financials. Key here is that these are for explanatory visualisation, they help convey the story or narrative. When interacting with a dashboard, it is about exploration, and so we can begin to understand how a dashboard can help achieve both focus-and-context, or overview and detail. Many dashboards may also “link” visualisations — whereby interactions and selections in one chart may update all related charts. Combining interaction across different linked visualisations makes dashboards a very powerful and effective form of exploration.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/460/0*MmHLTnXaWR_hROFg.png"/></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/425/0*vbFYXsOpA9B-bf6X.png"/></figure> <p>We introduced Plotly in our visualisation practical, and more recently, Plotly Dash has become <a href="https://plotly.com/dash/">“the most downloaded, trusted framework for building ML &amp; data science web apps.”</a> It is popular since it is written in Python, yet produces rich HTML-based interactive visualisations, without the need for further languages such as Javascript.</p> <h3>User Interaction</h3> <p>Humans learn by “doing” — it is the same in visualisation. It allows users to examine how something works, and specifically, it allows us to assess the strengths and weaknesses in our analysis. Typical interactions may include parameter selection (e.g., selecting a date range), filtering and selection (e.g., selecting a particular group of users), brushing (e.g. selecting a particular region on an axis), reordering (e.g., changing the data on an axis), and zoom (e.g., increase detail for a given region on a map). It is useful to think about interaction components such as those used in HTML 5 such as radio buttons, checkmark buttons, sliders, buttons, date time selections, as well as operations such as click, drag, hover, and focus. As this course primarily uses Python Notebooks, we can incorporate <a href="https://ipywidgets.readthedocs.io/en/stable/examples/Widget%20Basics.html">“Widgets”</a> such as sliders, text entry, drop-down menus, radio buttons, buttons, date and colour pickers, file upload dialogs, tabs, and accordions, and <a href="https://ipywidgets.readthedocs.io/en/stable/examples/Widget%20List.html">many more</a> directly into our notebooks to interact with Pandas. We will explore how to achieve this further in our practical work.</p> <p>A number of interaction methods have evolved in recent times: mouse and keyboard, stylus pen, touchscreen, hand gestures (such as Leap Motion and Kinect), and voice. When designing systems, we need to be mindful of the intended mode of operation, and the associated costs of interactions. Amazon Alexa is a prime example of voice command — it allows hands-free operation of a system to perform tasks such as playing music and listening to weather reports. However, it may be less appropriate to inspect a parallel coordinates plot via voice — it may be difficult to articulate exactly what interaction is required, or it may be that the commands are simply too long for them to be convenient any more. Likewise, hand gestures work well for video games that may be about an immersive experience, but wouldn’t neceessarily lend themselves well for searching a database. We have to think about tiredness and fatigue caused by any interaction method, and the complexity of the interactions. Mouse and keyboards remain most common in many computing applications, due to their familiarity for users. Yet, users who have been primarily exposed to touchscreens and touch keyboard (especially on mobile devices) may find they can type quicker on a mobile than a traditional keyboard. Therefore, designing effective user interaction involves knowing the intended audience, and how they are expected to use the system, and for what duration. Some systems may seek to combine multi-modal interactions — such as voice and touch. The <a href="https://chi2021.acm.org/">ACM CHI conference</a> is a long-standing academic venue for research publications in the area of human-computer interaction.</p> <h3>Challenges in Visual Analytics</h3> <p>You may hear talk about “big data” — yet we need to ask what the actual investigation is that requires big data. Humans can not comprehend the complete dataset, and machine may not be able to “process” the complete dataset (whatever we may be referring to by the “process” here). Visual analytics supports memory externalisation such that the system can serve to “remind” the user of some detail when needed, rather than the user be expected to recall all previous observations of data. Similarly, users can not visualise all data at once. As system designers, we need to make decisions about the level of perception that is appropriate when data is being conveyed to the user (or at least, offer the user a means of adjusting this level). Scalability will continue to dominate visual analytics as a challenge — however, the power of the human is to identify the appropriate way of incorporating more scalable methods for data analysis — such as machine learning — to then support the investigation of the data.</p> <p>How do we account for semantics in our data? For example, think about document collections (e.g., e-mail conversations). A system can calculate metrics such as word count and word occurrences, but a computer would not understand the true definition of a word. Even if it can infer that similar words occur together, there is a need for human intervention to assess the appropriateness of the words used. We’ll explore this further in our Section on text analytics.</p> <p>Uncertainty is a fundamental challenge in data analytics. Data is gathered from a sensor (which could be a packet capturer, a video camera, a software tool or a hardware sensor). How do we know that the sensor is gathering data reliably? How can we use visual analytics to inform about our confidence in the data, or the uncertainty that may have been introduced, and how should users comprehend this information? If a sensor is 65% confident, how does the user make best use of this information? (e.g., Frenquentist vs Bayesian probabilities). There will also be uncertainty in our users — how can we ensure they will use tools in the way they are designed? How can we ensure they will act in the way that is intended (e.g., providing correct information). Could visual analytics help to identify security concerns based on how users interact with a system (either deliberately or accidentally).</p> <p>Finally, how do we evaluate a good visual analytics system? Many dashboards are developed based on intuition, or from templates, but how do we scientifically assess design choices made? There is much research in the area about how we develop guidelines for reproducable visual analytics systems, rather than working on the assumption of bespoke software development. The <a href="http://ieeevis.org/year/2021/welcome">IEEE VIS</a> community, and especially for us, the <a href="https://vizsec.org/">IEEE VizSec</a> community (Visualisation for Cyber Security), are two important research venues where this is an ongoing discussion. We show 5 examples of different visual analytics applications. When examining these, think about how design choices have been made, how they support multiple linked views for exploration, and how you may expect a user to interact with the system from start to finish. The pictures are linked where you can find the research papers, and range from topics of insider threat detection, through to online gaming, taxi trajectories, text analysis, and massive open online courses.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/394/0*L8caWExU6sU1PSU3.png"/></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/391/0*EzLZWZI1yY-78AbC.png"/></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/386/0*lJL3ErE1ItaoQTy9.png"/></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/405/0*aYjlDg2Xse47uQ8K.png"/></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/393/0*I0JVFEhFAANotP8b.png"/></figure> <h3>Further reading</h3> <ul><li><a href="https://ieeexplore.ieee.org/document/7503278">P. A. Legg, “Enhancing cyber situation awareness for Non-Expert Users using visual analytics,” 2016 International Conference On Cyber Situational Awareness, Data Analytics And Assessment (CyberSA), 2016, pp. 1–8, doi: 10.1109/CyberSA.2016.7503278.</a></li><li><a href="https://towardsdatascience.com/building-dashboards-using-dash-200-lines-of-code-ae0be08d805b">Agarwal, R. Building Dashboards using Dash. Towards Data Science (2020)</a></li></ul> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=d914156553cc" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">Security Data Analytics — 05 — Visualisation</title><link href="https://pa-legg.github.io/blog/2025/security-data-analytics05visualisation/" rel="alternate" type="text/html" title="Security Data Analytics — 05 — Visualisation"/><published>2025-06-22T10:56:29+00:00</published><updated>2025-06-22T10:56:29+00:00</updated><id>https://pa-legg.github.io/blog/2025/security-data-analytics05visualisation</id><content type="html" xml:base="https://pa-legg.github.io/blog/2025/security-data-analytics05visualisation/"><![CDATA[<h3>Security Data Analytics — Session 05 — Visualisation</h3> <p><strong><em>*** This material is session 5 from the previous Security Data Analytics and Visualisation course that I led at UWE until 2024. It is now shared for reference purposes. ***</em></strong></p> <h3>Visualisation Techniques</h3> <p>Here we will give a brief overview of different visualisation techniques, highlighting where they are effective and how they should be used.</p> <p><a href="https://bl.ocks.org/mbostock/3884955"><strong>Multi-series Line Chart</strong></a><strong>:</strong> Line charts are effective for time-series data, where the horizontal x-axis would denote time, and the y-axis would denote some numerical attribute. With colour-coding, we can depict multiple lines on the same chart, where the purpose is for comparison between two or more observed measures.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/467/0*y2AyT7vJ54jjf1_v.png"/></figure> <p><a href="https://bl.ocks.org/mbostock/3885304"><strong>Bar Chart</strong></a><strong>:</strong> This depicts numerical attributes for different discrete classes along the x-axis, rather than some continuous value. This could be some count obtained for various countries, or observations of different malware varients. Height of the bar denotes the numerical value.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/449/0*NGqaPZWgyRJQafrv.png"/></figure> <p><a href="https://bl.ocks.org/mbostock/3887118"><strong>Scatter Plot</strong></a><strong>:</strong> This is useful for comparing two different numerical attributes together, for example the number of bedrooms in a house and the associated house price. Another example may be the RAM and CPU usage of infected and benign workstations, for studying behaviour differences in a malware investigation. The scatter plot helps to identify the relationship between two indendepent variables, often referred to as the correlation between two variables. Highly correlated variables will share some statistical pattern (e.g., either that they increase or decrease together which would be a postive correlation, or that when one increases the other decreases — and visa versa — which would be a negative correlation).</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/411/0*5YRG1wla3fyXObQc.png"/></figure> <p><a href="https://bl.ocks.org/mbostock/4063269"><strong>Bubble Chart</strong></a><strong>:</strong> This is actually very similar to a bar chart, in that it shows different classes or categorical attributes, and it shows a numerical value associated with this — however here rather than a bar we use a bubble. Firstly, the plot is more visually appealing than a bar chart, and so may be more suitable for engaging the desired audience. Secondly, there is no sense of order — a bar chart runs left to right, whereas bubbles are dynamic and can place wherever, be it automatically or manually. Finally, bubble charts are arguably more compact than bar charts — where a very tall bar may result in lots of white space. However, bubble charts are known to be misleading. Firstly, is the numerical quantity mapped to the diameter, the radius, or the area of each bubble? In theory, you could use any of these geometric properties of the shape, resulting in different scaling of each entry. Secondly, how easy is it to compare one bubble to the next beyond whether it is larger or smaller, for example, how easily can I identify where an attribute to twice the value of another? In such scenarios the bar chart is more suitable — comparing two lengths when side-by-side is much easier than comparing two “bubbles” — be it by diameter, radius or area — when they are arbitrarily placed on the plot. As mentioned, the bubble chart is about creating an impactful visualisation that captures the attention of the audience, rather than providing a scientific analysis tool.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/406/0*K3QwAsfDdnXDYx_k.png"/></figure> <p><a href="https://bl.ocks.org/mbostock/4062045"><strong>Force-Directed Graph</strong></a><strong>:</strong> We discussed how a scatter plot can show relationships between attributes, however this tends to be for a one-to-one mapping between two variables. How do we show one-to-many relationships? Force-directed graphs help, where points are connected by edges, and there may exist a one-to-one or one-to-many relationship between points. A prime example is a social network graph, where nodes are people and edges are whether the two people know each other (i.e., are connected). The same can apply to computers on a network. They are described as force-directed since nodes are not positioned with a fixed x and y point like on a scatter plot, but instead, nodes are positioned using a physics-based force algorithm that will treat each connection like elastic where connected points will be pulled closer together. This force layout helps to further draw out relationships between nodes, especially when a high number of nodes are depicted. However, force-directed networks should be used carefully. In a situation where many nodes are all connected together, this creates strong forces pulling together and essentially forms a cluster — often described as a ‘hairball’. If you want to show many connected nodes together, you should consider how to best filter the data so that it can remain informative.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/524/0*T9KsPNHfhuFT3VEH.png"/></figure> <p><a href="https://bl.ocks.org/jasondavies/1341281"><strong>Parallel Coordinates</strong></a><strong>:</strong> Whilst scatter plots are useful, they are limited to 2 or 3 dimensional spaces. Can we visualise information that may be higher dimensionality? Parallel coordinates achieve this, where each vertical line represents a data axis (like the x and y of a scatter plot would), however rather than denoting a point on each axis, we use a line to connect a pair of axes. We do this for each pair as the axes are displayed, so that a single data instance is essentially depicted by a line that crossed each axis. We can identify correlations between pairs of neighbouring axes — do the lines all go up or down together, or do they cross over? As in other chart types, we can colour code by class. We can also filter (or brush) axes, much like we may set a region of a scatter plot. Parallel coordinates are therefore very effective — however they may be unfamiliar to some users, and so there is a learning curve to overcome. Another limitation is that correlations can only be assessed on neighbouring axes — some implementations allows axis reordering however this can create additional overhead for the analyst.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/417/0*5US22-0-ol9eMy6t.png"/></figure> <p><a href="https://bl.ocks.org/mbostock/4063582"><strong>Treemap</strong></a><strong> and </strong><a href="https://bl.ocks.org/kerryrodden/766f8f6d31f645c39f488a0befa1e3c8"><strong>Sunburst</strong></a><strong>:</strong> Treemaps are excellent for depicting hierarchy. They were initially developed for conveying information about computer files and folder structure. Each entry is scaled based on the numerical value being considered (e.g., file size). Since a group of files may exist within the same folder, we can group them together and therefore show the collective size of the folder also (shown either by a bounding box, or by colour coding). Since the full rectangular area denotes the full space available (e.g., the entire hard drive), it can show size in relation to the full disk, and means that available space is also shown (essentially as a blank area of the chart). The sunburst is a similar concept, except it is mapped to a circle rather than a rectangular area. The hiararchical structure works outwards in the sunburst, and so is not as compact as the treemap, however, it does show proportion well just like the treemap and it does so in a striking and visually appealing way, therefore making it good for audience engagement. Some modern operating systems (e.g., Ubuntu) use the sunburst to show disk space analysis.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/367/0*OZ4BQr2lgr5y7vtI.png"/></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/319/0*265cJ3YlHrEWGne7.png"/></figure> <p><a href="http://bl.ocks.org/kevinschaul/8213691"><strong>Star Plots</strong></a><strong>:</strong> The final visualisation type we will consider here is the star plot. These essentially map multiple attributes in a small and compact format, often referred to as a glyph (where a glyph is a small depiction of multi-variate data). In this example, 8 attributes are mapped in each glyph. The mapping is similar to how a parallel coordinates plot works, except here the layout is radial rather than left-to-right.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/428/0*lwaKNroGHKLD2umq.png"/></figure> <h3>Visualisation for Cyber Security</h3> <p>Having introduced differe forms of visualisation, here we will now look at how these can be used for the purpose of cyber security. Of course, this is not an exhaustive set, but will help to illlustrate the effectiveness of visualisation techniques for analysing large complex data, which is essentially what we aim to do as part of understanding and defending large corporate networks. For more details, and further examples, I recommend looking at the two books: Applied Security Visualization by Raffael Marty and Security Data Visualisation by Greg Conti, as well as the primary text for the course.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/434/0*5pYY2j2M-1kUPbMB.png"/></figure> <p>In this example, we see parallel coordinates being used to depict network traffic across a set of 7 attributes. This image shows a comparison between regular wireless network traffic, compared against a WEP key cracking attack against the network. We can see the difference between the two activities clearly in the two images, where essentially the WEP cracking attack is scanning all available ports and uilising a single protocol. The approach is detailed further in the paper, <a href="https://ieeexplore.ieee.org/document/5718652">“Visualizing Networking Activity using Parallel Coordinates”</a> by Tricaud et al.</p> <p>In the paper <a href="https://pdfs.semanticscholar.org/66b8/1e4fe80511154703385d5d4ab2abc6164140.pdf">“Fast detection and visualization of network attacks on parallel coordinates”</a> by Choi et al., they propose the use of parallel coordinates for network traffic analysis, but use this to define small glyphs that are indicative of network behaviours. The distinguishable shape of the data plots can be treated as a signature here, to easily recognise behaivours such as worm, port scan, or DDoS.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/428/0*LdYU9MFIIzb9WTWK.png"/></figure> <p>As mentioned earlier, similar to this is the star glyph, which maps axes in a radial manner to create a connected polygon, where the length from the centre to each point denotes a variable. In the example shown here, we have 8 attributes about network packet captures mapped, and so we can show individual packets as glyphs for comparison. Glyphs are widely used in various applications, for example, insider threat detection. This example shows 18 individuals from a company and their behaviours during a 12 month period. Even with such volume data, some differences can be identified (suspicious cases are highlighted with the grey circle, two of these users are denoted in blue as potentially malicious).</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/453/0*y1jye2giDBkUM2ku.png"/></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/308/0*B-3I-eo8bgcW4EeO.png"/></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/279/0*xfN0_BMK7IGVPRy7.png"/></figure> <p>We have mentioned node-link diagrams (also referred to as force-directed) for social media analysis and network profiling. Here we show three images to convey this further. In particular, we can see the complexity of the node-link diagrams as more nodes are included, and where a central node has many connections, we start to see the hairball effect mentioned earlier. This is where good visual analytics planning is required to have appropriate forms of filtering and selection to make the chart useable.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/316/0*__JKb2ll0dwKRPe0.png"/></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/252/0*PHWojTMVVH1R4BHN.png"/></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/337/0*E7-F6gJGQuuyAQd0.png"/></figure> <p>Treemaps were discussed earlier, and here we see snort alerts mapped against a tree map to show the volume of alert types, where alerts will naturally exist as part of a group (i.e., within a hierarchy).</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/562/0*oBigCebTi4snX79_.png"/></figure> <p>A final example to consider is the use of visualisation for binary file analysis (we will discuss this in further detail later in the course). Greg Conti shows an excellent example of this, where binary data is mappped to pixel values to produce an image of the data. We can examine what the same image may look like using different image compression schemes (e.g., bmp, png, gif, jpeg), as well as how a Microsoft Word document may appear once password-protected or encrypted (here we see that the password-protected file does not encrypt the original data).</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/457/0*WbDB6Z-PwjU0dV0O.png"/></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/232/0*RmCeFQQYw7PQ1vRL.png"/></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/440/0*Bkkbr_7L4BhBdo-F.png"/></figure> <h3>Visual Channels</h3> <p>The final point to cover in this session is on visual channels. All visualisations are made up of visual channels, and so we need to ensure that these channels are used effectively. Some examples of channels would be colour, size, orientation, shape, texture, opacity. Spatial positioning is also a visual channel — for example how data is mapped to an axis or positioned within a space. How do we know which channels should map to which data attribute? There are no fixed rules as such — but as we discussed some of the weaknesses in visualisations earlier, we need to consider what may work best for our data. Mapping a single numerical value to a circle for example can cause confusion. We need to think about the type of data being shown. There are 4 fours we need to recognise: nominal, ordinal, interval, or ratio. Nominal data can be text labels (e.g., name) or categories (e.g., car type) — there may not be a specific order to these — we may use alphabetical ordering but that is purely for convenience. Ordinal data does have some order to it, however the difference between each possible value is unknown or not exact (for example, a threat level scale or a likert scale). Interval data is ordered and the interval between each data point is known — for example, temperature values. Ratio data is the same, however it has an absolute zero. Temperatures can be negative, however a packet size or a count of some data would be a ratio as you would not have a negative value for this.</p> <h3>Further reading</h3> <ul><li><a href="https://ieeexplore.ieee.org/abstract/document/7312772">P. A. Legg, “Visualizing the insider threat: challenges and tools for identifying malicious user activity,” 2015 IEEE Symposium on Visualization for Cyber Security (VizSec), 2015, pp. 1–7, doi: 10.1109/VIZSEC.2015.7312772.</a></li></ul> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=492820fd7cd0" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">Security Data Analytics — 04 — Machine Learning</title><link href="https://pa-legg.github.io/blog/2025/security-data-analytics04machine-learning/" rel="alternate" type="text/html" title="Security Data Analytics — 04 — Machine Learning"/><published>2025-06-22T10:55:49+00:00</published><updated>2025-06-22T10:55:49+00:00</updated><id>https://pa-legg.github.io/blog/2025/security-data-analytics04machine-learning</id><content type="html" xml:base="https://pa-legg.github.io/blog/2025/security-data-analytics04machine-learning/"><![CDATA[<h3>Security Data Analytics — Session 04 — Machine Learning</h3> <p><strong><em>*** This material is session 4 from the previous Security Data Analytics and Visualisation course that I led at UWE until 2024. It is now shared for reference purposes. ***</em></strong></p> <h3>What is Machine Learning?</h3> <p><a href="https://expertsystem.com/machine-learning-definition/">Machine learning</a> is a domain within artificial intelligence (AI) that provides systems with an ability to learn from experience of data observations <strong>without being explicitly programmed</strong>. What this means is that we do not need to explicitly program the rules that may help us to differentiate between categories within our data, instead we determine how the data will be presented (i.e., what are the features of the data), and we allow the machine to determine a way of organising this information. Machine learning focuses on the development of computer programs that can access data and use it learn for themselves. We can essentially think of three components: an input (the data), a function, and an output. The machine is trying to learn the function that can map inputs to some form of output, which is achieved by minimising some <strong>error measure</strong> (sometimes described as maximising a fitness function). One possible approach is to consider the difference between the expected output and the predicted output as the error measure.</p> <blockquote><em>A decision model that is not explicitly programmed, but instead is learnt through the training of example data.</em></blockquote> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*lOojO1zPvnwsdkhx.gif"/></figure> <p><strong>Supervised Learning:</strong> This is where we have a set of data observations, and we provide labels about each of these observations to inform the machine what the data essentially represents. For example, if we have a set of images that we wish to classify into groups, we can state that the output is what the image depicts (for example, is it a cat or a dog?). The input to the system is the raw image data assessed based on the colour pixel intensity values. The output of the system is a label of either ‘cat’ or ‘dog’. The function that converts pixel data to a label is what the machine attempts to learn. This is often described as <strong>classification</strong> since we are classifying the data into a series of groups. This exact same concept can be extended to many other applications, such as classifying malicious and benign software, or malware families, or even classifying types of users of a system.</p> <p><strong>Unsupervised Learning:</strong> This is where data is provided much like before however no training labels are given. Leading from our earlier example then, we may have image data of cats and dogs, however we do not have each image labelled as such. Here, the system attempts to learn a suitable way of <strong>clustering</strong> the data, under some mathematically assumptions. As an example, this could be distance-based, where it may be assumed that observations of the same type may appear close together in feature space. A scatter plot could then be used to explore the learnt separation of the data, to assess whether the groupings of the data make sense from some intuitive perspective. This can be a really powerful technique for dealing with a large volume of data fairly quickly, where labelling each data observation is not an option due to time constraints. Furthermore, some approaches may combine unsupervised learning as an initial data processing stage, to then support more efficient labelling to inform a supervised process.</p> <p>Other forms of learning also exist, such as <strong>semi-supervised</strong> (where the system may only be informed on a subset of samples, as identified through some means which could be unsupervised), <strong>active learning</strong> (which involves a human-in-the-loop to label a small subset of data based on some underlying data attributes), and <strong>reinforcement learning</strong> (which is learning based on trial-and-error, popular for learning to play video games or robotic navigation).</p> <p>Important to discuss also is the notion of training and testing. Typically, we develop a machine learning classifier based on training data. This is data that is representative of the problem domain that we are addressing, and our system may observe this data many times as part of adjusting the function that is calculated to map inputs to outputs correctly. Once we have completed sufficient iterations of this, such that the accuracy of mapping inputs to outputs on the training data is relatively high (ie., a high accuracy score is achieved), we would then test the model using a test dataset, that contains samples that were not in the training data, but are drawn form the same distribution. If we can achieve a high accuracy using the testing data also, then our model can be considered to be <strong>generalisable</strong>. Where the model performs high for training data but low for testing data, this can be described as <strong>overfitting</strong> our model to the training data. This is an important concept to grasp as the purpose of machine learning is to develop a system that can generalise to new data observations, and predict outputs on these, based on historical data patterns.</p> <h3>Search Optimisation</h3> <p>Earlier we introduced the concept of minimising some error measure, or error function. We can think about this as trying to find some minimum point on a curve — the challenge is that we do not know the minimum point ahead of time. Suppose we are at the yellow point on the curve, and we can look either one place left or right. By stepping along we can find the minimum point on the curve, however the notion of step size is important to realise. Too small a step size would mean that the time taken is too large. Too large a step size would mean that we overstep our intended solution. Furthermore, in most challenges the curve may not be a simple as a standard U-shape. Imagine a rock face, where there are ridges and nooks along the surface. We can think of these as local minima, where the point either side is greater — and so therefore have we found the mimimum point? The true mimimum is often described as the global minimum. Search optimisation aims to find the global minimum solution, in a way that can traverse this space efficiently, and can overcome local minima that may conceal the correct solution.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*zFoEGj3mhdka5mU0.png"/></figure> <h3>Methods of Learning</h3> <p><a href="https://en.wikipedia.org/wiki/Linear_regression"><strong>Linear Regression</strong></a><strong>:</strong> Widely used in statistics, linear regression is an approach for modelling the relationship between a dependent variable and one or more explanatory, or independent, variables. This essentially gives a continuous value prediction of some outcome, based on a set of numerical inputs. For example, imagine I want to predict house prices in my local area. If I know number of bedrooms and listed price, I can develop a mathematical model to express this, where number of bedrooms is the input, and price is the output. For any number of bedrooms within my distribution, I can then predict the output price. Of course, the model may be wrong, and would likely require more attributes, such as garden size, access to schooling, whether there is a driveway, and many other factors. We see here how multiple inputs variables will inform a single output value. I could model whether a network packet is malicious or benign in a similar fashion, where multiple inputs inform a single numerical output. The important thing to note here, is that this will only model a linear relationship — i.e., there is strong linear correlation between the inputs and the output, and so this is often too simplistic for many real-world challenges, but does at least provide a useful starting point. Linear regression can be expressed in the form: y = mx + c, where m is the gradient of the line, and c is the y-intercept of the line. We can calculate the Mean Squared Error (MSE) to measure how closely the data fits the line, where the smaller this value is, the closer the fit.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/400/0*WYSEcnvFcQc69LmC.png"/></figure> <p><a href="https://en.wikipedia.org/wiki/Cluster_analysis"><strong>Clustering</strong></a><strong>:</strong> Here we want to observe when different observations form groupings of similar behaviour. We have discussed earlier the idea of clustering malware as malicious or benign. Similarly, we may have some data that is unlabelled, and we want to assign some labels to this. A popular method for this is known as k-means clustering, which will aim to identify k unique clusters within a given dataset, where data points are grouped together based on their collective mean values. Essentially, we begin with k randomised points, and for each point within our data, we determine which of our k points the data is closest to, so that we can assign a label. Once we have assigned a label to all data points, we update our k points so that each k point is the centroid of the group — which essentially is the mean in both the x and y axis, assuming we are dealing with 2 attributes in our data. We continue this process until we find a solution where the k points no longer change, and therefore our solution has stablised. We could essentially say we have minimised the error (the distance required to update the values). The practical session will delve into clustering further.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/287/0*cSGCN7-egnWdLwcV.png"/></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/305/0*DAuV51pHi-JTehd3.png"/></figure> <p><a href="https://en.wikipedia.org/wiki/Support_vector_machine"><strong>Support Vector Machines</strong></a><strong>:</strong> Similar to how we have described our clustering approach above, SVM works through iterative search to find an optimal solution. Likewise, just as linear regresssion provides a line-of-best-fit through the data, SVM aims to fit a line that provides a decision boundary so that data points can be classified. Specifically, we measure the <a href="https://en.wikipedia.org/wiki/Euclidean_distance">euclidean distances</a>between data points nearest to the decision boundary. The objective is to maximise the distance between points and the decision bounary. The euclidean distance essentially measures the distance as a diagonal distance (think about the long side of a triangle, and how you would calculate this distance using Pythagorean Therorem). We can use the same formula as described earlier for linear regression, y = mx + c, to characterise the decision boundary line.</p> <p><a href="https://en.wikipedia.org/wiki/Dimensionality_reduction"><strong>Dimensionality Reduction</strong></a><strong>:</strong> Earlier we commented that our search optimisation may be in some higher dimensional space. We often use 2D or 3D visualisations to convey them simply because we can not easily observe dimensionlity higher than this. Yet, some dataset that has 100 different numerical attributes about each data instances could be considered to be a high dimensional dataset (100-dimensional). Is there a meaningful way that we can visualise this data to identify data instances that mayshare similarities, or at least may make it easier to summarise the key characteristics of the data instance. Dimensional reduction aims to achieve this, such that we can preserve the structure of the data in a low dimensional space that can be easier understood and visualised.</p> <p>Principal Component Analysis (PCA) is a widely used method of dimensionality reduction. For a given dataset, it essentially seeks to find the direction (or vector) of greatest variance through the data. Suppose we have some points in 2-dimensions. We can fit a line through this data, much like we have seen already. Then, we could treat this line as a new “axis” for our data, and essentially map our points against where they occur on this line. We could also imagine a similar approach if we wanted to map 3-dimensional points to 2-dimensions, by finding a plane that all points could be mapped on to (where we say plane, imagine this as a flat piece of paper being held at some orientation within our 3-dimensional points, and then placing a dot on our page where each point occurs). Other methods of dimensionality reduction include t-distributed stochastic neighbourhood embedding (t-SNE), and uniform manifold and projection (UMAP). Dimensionality reduction is useful as part of unsupervised learning, where we want to cluster data but have no training labels to inform our process.</p> <p><a href="https://en.wikipedia.org/wiki/Artificial_neural_network"><strong>Neural Networks</strong></a><strong>:</strong> Probably the most commonly described form of machine learning in use today is the neural network. The name derives from the notion of synapses in the brain firing when they receive a signal, where many signals contribute towards whether a synapse should fire or not. Here, we have an input layer (red) which takes in some attributes about our data — for example, a set of numerical features that have been derived. For classification, the output layer would relate to the possible output classes of the data, where the highest scoring node would be the predicted class. In between are a set of hidden nodes that the machine will attempt to learn a suitable numerical value for, such that input nodes map to the expected output nodes. We do this using <a href="https://en.wikipedia.org/wiki/Matrix_multiplication">matrix multiplication</a>. We will show this further in the practical session. The important detail to know is that neural networks are calculated as <em>y = sigmoid (Wx + b)</em>, where <em>x</em> is my input vector (e.g., 5 numerical values in a 5-by-1 column), <em>W</em> is a weights matrix (where the matrix has the same number of columns as the number of rows in our input vector), and <em>b</em> is a bias vector that is added as a constant. The sigmoid function then scales our resulting vector so that our values remain within our set bounds (either -1 and +1, or 0 and 1). What we have described is often considered as a feed-forward network. Since our model is likely to incorrectly predict our data initially, we need a method for updating the weight values in our hidden nodes, since these essentially dictate how our input relates to our output. This is done using a technique called <a href="https://en.wikipedia.org/wiki/Backpropagation">backpropagation</a>. This essentially works like the search optimisation technique described earlier, where we seek to minimise the distance between the predicted values and our expected values, which is achieved through the process of training the neural network.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/469/0*hM7nVXlwo43SveAn.png"/></figure> <p>For our purpose here, we want to understand what different techniques exist for machine learning. Here we are not going to focus too much on the underlying mathematics used, although it is important to understand how these methods operate and why. Neural networks have formed the basis of much modern machine learning research. The concept of <a href="https://en.wikipedia.org/wiki/Deep_learning">Deep Learning</a> comes from having many hidden node layers in a neural network structure, such that it is a deep neural network. Furthermore, there now exist a wealth of different architectures based upon the notion of deep learning. <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">Convolutional Neural Networks</a> have become particularly popular for image analysis tasks. This is because instead of having single vector inputs, they use square regions as input, which means they are much better at preserving spatial attributes such as the relationship between pixels in an image. The final method to mention here is <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">Recurrent Neural Networks</a>, which are designed to work on sequential forms of data. Whereas our previous learning models have treated each data instances as a discrete observation, in a recurrent model, each instance of data is dependent on what came before. A common example is language modelling — so similar to subsequence anomalies discussed in Section 3, our model could learn that the input phrase “The quick brown fox jumps over the lazy” is completed by the word “dog”.</p> <p>We have covered a lot of ground in this section, but hopefully this helps to give you a brief introduction to the diferent machine learning models that are used. In particular, we have seen how each model takes an input, performs some processing or operation, and provides an output. We have also seen how this is based on minimising or maximising some function over time through the process of training to achieve the final output. It is important to think about the task to be solved as to which method is most appropriate — are we hoping to predict a class (classification), a continuous value (regression), a group (clustering), or some complex non-linear function (neural network)?</p> <p>As a slightly aside, when using machine learning for the purpose of cyber security, it is vital that we consider the case where an adversary is able to manipulate the performance of our model. You may have seen examples of this, where image recognition systems can be made to misclassify objects, despite a well-trained model being used. Think about the impact here for up-and-coming domains such as autonomous vehicles and healthcare. This is why when we think about AI for cyber security it is fundamental that we also think about the cyber security of AI. We are merely scratching the surface here, however there is much research activity in the area of <a href="https://en.wikipedia.org/wiki/Adversarial_machine_learning">adversarial learning</a>.</p> <p>We have covered a lot in this section, and it is worth taking the time to reflect on the different techniques used in machine learning for cyber security. The practical sessions that accompany this chapter will show you how the implement clustering, linear regression, and neural networks, using Python and the Jupyter Notebook environment, to give you a feel for how the inner workings of these concepts perform. This will also help develop your understanding when using off-the-shelf libraries such as scikit-learn, to understand how the algorithms scale up and perform on larger datasets.</p> <h3>Further reading</h3> <ul><li><a href="https://www.sciencedirect.com/science/article/pii/S0167404818305546">M. Rhode, P. Burnap and K. Jones, “Early-stage malware prediction using recurrent neural networks”. Computers &amp; Security, Volume 77, August 2018, Pages 578–594.</a></li><li><a href="https://ieeexplore.ieee.org/abstract/document/8899533">A. Mills, T. Spyridopoulos and P. Legg, “Efficient and Interpretable Real-Time Malware Detection Using Random-Forest,” 2019 International Conference on Cyber Situational Awareness, Data Analytics And Assessment (Cyber SA), 2019, pp. 1–8, doi: 10.1109/CyberSA.2019.8899533.</a></li></ul> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=5edef0e764f0" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">Security Data Analytics — 03 — Statistical Analysis</title><link href="https://pa-legg.github.io/blog/2025/security-data-analytics03statistical-analysis/" rel="alternate" type="text/html" title="Security Data Analytics — 03 — Statistical Analysis"/><published>2025-06-22T10:54:59+00:00</published><updated>2025-06-22T10:54:59+00:00</updated><id>https://pa-legg.github.io/blog/2025/security-data-analytics03statistical-analysis</id><content type="html" xml:base="https://pa-legg.github.io/blog/2025/security-data-analytics03statistical-analysis/"><![CDATA[<h3>Security Data Analytics — Session 03 — Statistical Analysis</h3> <p><strong><em>*** This material is session 3 from the previous Security Data Analytics and Visualisation course that I led at UWE until 2024. It is now shared for reference purposes. ***</em></strong></p> <h3>Feature Pre-processing</h3> <p>In the last section, we talked about how you can combine and transform data for the purpose of analysis. How do we achieve this? We need to have a clear understanding of what our input data is, and what we hope to achieve for our output, such that this can help inform our investigation. Therefore, we need to think about the input, the process, and the output.</p> <p><strong>How do we construct features from our data?</strong> Essentially features can be thought of as numerical values, and quite often, we may just be counting data. If I wanted to derive a feature to measure RAM usage, or email usage, I would need to specify some time interval to observe (e.g., per hour, or per day). Much like when reporting speed, we would refer to miles per hour, rather than recording the absolute speed of the vehicle at every observation. In this way, we generalise to the time interval that makes most sense in practice (we could examine miles per minute, but it’s more natural to state 70 mph rather than 1.67 miles per minute). If I’m studying email usage, rather than simply just the number of emails sent, I may start to form some classifications of my features — for example, number of emails sent to unique recipients, number of emails sent to a specific individual, number of new recipients per day, number of words in each email, number of unique words in each email, and the list goes on. Hopefully you can start to see that there are many possible ways that we could derive numerical count features about email (and many other data observations). Crucially to remember is that we are interested in observations over time, so that we can compare time periods to understand where there may be an increase or decrease in the observed measure. As well as temporal features, we may be interested in spatial features — such as pixel locations in an image or GPS points on a map, or furthermore, we may be interested in sptiotemporal features — such as a pixel location in a video stream or a moving vehicle position.</p> <p><strong>Finding and cleaning data?</strong> There are a number of excellent resources for gathering example datasets, such as <a href="https://www.kaggle.com/">Kaggle</a>, the <a href="http://www.vacommunity.org/About+the+VAST+Challenge">VAST Challenge</a>, the <a href="https://archive.ics.uci.edu/ml/index.php">UC Irvine Machine Learning repository</a>, and numerous other examples hosted online and in various data repositories. Whilst these are useful for learning about machine learning and visualisation, much of the hard work has already been done for us. <strong>Web scraping</strong> is often used to gather large amounts of data from online sources, for example, news story analysis, or examining CVE records. In such cases, there will be significant cleaning of data required, such as filtering out noise in the collected data, or correcting timestamps so that they are reported consistently. We will look at methods for cleaning data in our example practicals.</p> <h3>Types of Anomalies</h3> <p><a href="https://www.datasciencecentral.com/profiles/blogs/anomaly-detection-for-the-oxford-data-science-for-iot-course">Anomaly detection</a> is widely discussed in terms of cyber security, and how “artificial intelligence can spot anomalies in your data”. However, it’s crucial to understand what we mean by anomalies, and what type of anomalies may exist. This will help us to understand what the anomalies actually mean to assess whether they pose some form of security concern. Here we will focus primarily on 3 types of outlier: point, contextual, and subsequence. In most applications here, we are thinking about <a href="https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2">time-series anomalies</a>: essentially how something changes over time.</p> <ul><li><strong>Point anomaly:</strong> This is where a single point in a time series is anomalous compared to the rest of the data. This is the most typical kind of anomaly that we may think of, yet if we are graphing the correct data, it is also the most straightforward to identify.</li><li><strong>Contextual anomaly:</strong> This is where a data instance in a time series is considered anomalous because of the context of the data. If we were measuring the temperature of different locations, and one location in the northern hemisphere reported low temperatures in the Summer, this may be an anomaly. Note that the temperature data alone is not sufficient to recognise this — we would need to have prior knowledge of temperature data for countries in the northern hemisphere during the Summer months, gathered historically, to be able to inform on the context here. Another example would be the presence of malware running on an infected machine, and the impact on CPU usage and process count. To recognise the anomaly here, we would need to know what the “typical” CPU usage and process count are in a state where the machine is deemed to be acting normally. In this manner, the anomaly is identified with respect to the historical data, where this observation may be much higher or much lower than the previous records. This historical data may be informed from some database of known anomalous (and non-anomalous) cases, or it may be informed directly from the data itself, where a repeating pattern is expected (e.g., seasonal).</li><li><strong>Subsequence anomaly:</strong> This is where a sequence of individual events are deemed to be anomalous with regards to the rest of the data, although the individual data points themselves are not deemed as anomalous. This could be seen as similar to contextual anomalies, however the key difference is that subsequence anomalies may not be out-of-distribution, whereas contextual anomalies would be. For example, a recurring pattern that then suddenly flattens for a period, and then begins again, would be recognised as an anomaly. Yet, each individual data point is well within the prior distribution of the data. It is only anomalous because the sequence, or pattern, is anomalous. Consider another example related to insider threat detection. An employee may conduct the following steps in an activity: (1) log into payment application, (2) retrieve payment details, (3) record item to be purchased, (4) enter payment details, and (5) send email to line manager. Each individual step may not be anomalous on their own (i.e., all legitimate actions). Likewise, they may be authorised to make purchases at any time of day, as needed. However if something changed in this sequence — e.g., suppose they stopped emailing line manager following payment, stopped recording item to be purchased; or added a new step — such as open notepad, and write purchase details to file — then there would be a cause for concern. As mentioned, here each individual activity is legitimate, however it is about observing a anomalous sequence of events, rather than an individual anomaly. Note how subsequence anomalies can be used for both numerical and discrete data — such as labels. Another example for text analytics could be, “The quick brown fox jumps over the lazy camel”. Many people will be familiar with this phrase, and will therefore recognise that the word camel is an anomaly — not because camel would necessarily be an incorrect statement, but because the well known quote would say ‘dog’.</li></ul> <figure><img alt="" src="https://cdn-images-1.medium.com/max/670/0*SMd5-FSqHKESOOI6.png"/></figure> <h3>Descriptive Statistics</h3> <p>Statistics are at the very heart of understanding the properties of data. There are some <a href="https://elearningindustry.com/stats-101-need-know-statistics">core concepts</a> that you should therefore understand. Firstly, when we talk about a set of data, we may refer to this as a distribution — it is a set of measured observations that are indicative of real-world. The Mean is the average value of the distribution — for example, if I was assessing the number of network packets received per minute, then the mean would be the average number of packets received per minute. This could be used to estimate a baseline for the activity (i.e., the expected behaviour). The Median would then be the middle value of the distribution, if I arranged all values from lowest to highest. The Mode is the most common value that has occurred in the distribution. Each of these gives us some indication of where the centre of our data lies — however each has its own weaknesses. If there are outliers in the data, the mean will be skewed by these — so a single point anomaly can change the mean completely. With the median, essentially only the first half of the data is counted (i.e., if I have n values I count up to the n/2 value) which means that the higher values are completely ignored. Therefore, it is good practice to assess all measures in case they help inform different stories about the data. Standard deviation is also an important measurement to understand. This informs about the spread of the data — whether it is narrow around the centre point, or spread out across the range of values (the range being essentially the difference between the largest and smallest values). In many applications, it is useful to consider the normal distribution (sometimes referred to as a bell curve, or a Gaussian distribution). The normal distribution can be expressed by the mean to define the centre point, and the standard deviation to define the spread. This becomes particularly useful when we want to consider whether a new observation is deemed to be inside or outside of the distribution, since approximately 95% of the data observations should be within 2 standard deviations of the mean.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*HbNaUAprMDs-dtYm.png"/></figure> <h3>Comparing Data</h3> <p>Suppose we have four datasets that we wish to compare, to observe any deviations or anomalies that may occur. How may we approach this task? Let’s assume that each dataset has two parameters (X and Y). We therefore have X1 and Y1 as dataset 1, X2 and Y2 as dataset 2, X3 and Y3 as dataset 3, and X4 and Y4 as dataset 4. We can use the code below to load in our sample dataset.</p> <pre>import pandas as pd</pre> <pre>data = pd.read_csv(&#39;./data/anscombe.csv&#39;)<br />data</pre> <p>X1Y1X2Y2X3Y3X4Y40108.04109.14107.4686.58186.9588.1486.7785.762137.58138.741312.7487.71398.8198.7797.1188.844118.33119.26117.8188.475149.96148.10148.8487.04667.2466.1366.0885.25744.2643.1045.391912.5081210.84129.13128.1585.56974.8277.2676.4287.911055.6854.7455.7386.89</p> <p>First of all, we can calculate statistics for each of the four datasets to see how they may vary. We will try a set of common statistics in the next few cells, starting with the mean of both X and Y parameters, the variance of X and Y parameters, the correlation between X and Y parameters, and finally the line of best fit, or the regression line, of our X and Y parameters.</p> <pre>print (&quot;Mean of X data:&quot;)<br />for i in [&#39;X1&#39;, &#39;X2&#39;, &#39;X3&#39;, &#39;X4&#39;]:<br />    print (data[i].mean())</pre> <pre>Mean of X data:<br />9.0<br />9.0<br />9.0<br />9.0</pre> <pre>print (&quot;Variance of X data:&quot;)<br />for i in [&#39;X1&#39;, &#39;X2&#39;, &#39;X3&#39;, &#39;X4&#39;]:<br />    print (data[i].var())</pre> <pre>Variance of X data:<br />11.0<br />11.0<br />11.0<br />11.0</pre> <pre>print (&quot;Mean of Y data:&quot;)<br />for i in [&#39;Y1&#39;, &#39;Y2&#39;, &#39;Y3&#39;, &#39;Y4&#39;]:<br />    print (data[i].mean())</pre> <pre>Mean of Y data:<br />7.500909090909093<br />7.50090909090909<br />7.5<br />7.500909090909091</pre> <pre>print (&quot;Variance of Y data:&quot;)<br />for i in [&#39;Y1&#39;, &#39;Y2&#39;, &#39;Y3&#39;, &#39;Y4&#39;]:<br />    print (data[i].var())</pre> <pre>Variance of Y data:<br />4.127269090909091<br />4.127629090909091<br />4.12262<br />4.123249090909091</pre> <pre>print (&quot;Correlation between X and Y:&quot;)<br />for i in [[&#39;X1&#39;,&#39;Y1&#39;], [&#39;X2&#39;,&#39;Y2&#39;], [&#39;X3&#39;,&#39;Y3&#39;], [&#39;X4&#39;,&#39;Y4&#39;]]:<br />    print (data[i].corr())</pre> <pre>Correlation between X and Y:<br />          X1        Y1<br />X1  1.000000  0.816421<br />Y1  0.816421  1.000000<br />          X2        Y2<br />X2  1.000000  0.816237<br />Y2  0.816237  1.000000<br />          X3        Y3<br />X3  1.000000  0.816287<br />Y3  0.816287  1.000000<br />          X4        Y4<br />X4  1.000000  0.816521<br />Y4  0.816521  1.000000</pre> <pre>from sklearn.linear_model import LinearRegression<br />for i in [[&#39;X1&#39;,&#39;Y1&#39;], [&#39;X2&#39;,&#39;Y2&#39;], [&#39;X3&#39;,&#39;Y3&#39;], [&#39;X4&#39;,&#39;Y4&#39;]]:<br />    lm = LinearRegression() <br />    lm.fit(data[i[0]].values.reshape(-1, 1), data[i[1]].values.reshape(-1, 1))<br />    print(lm.coef_)</pre> <pre>[[0.50009091]]<br />[[0.5]]<br />[[0.49972727]]<br />[[0.49990909]]</pre> <p>Having performed our initial analysis, what can we observe about our four datasets. What is particular intriguing in this example, is that all four datasets have exactly the same statistical characteristics! They all show the same mean for both X and Y, the same variance, correlation, and regression line. So, presumably are these datasets essentially all the same then?</p> <p>This is a perfect case for data visualisation, and is actually a well-known problem known as Anscombe’s Quartet. When we visualise the data using four scatter plots, we can quickly determine that the four datasets are wildly different. However, on the surface, the descriptive statistics gave the same information. As data becomes increasingly large, we do need to use statistical measures and these are important, but it is also important that we do not rely on them solely. <a href="https://www.sjsu.edu/faculty/gerstman/StatPrimer/anscombe1973.pdf">Anscombe</a> (1973) said:</p> <blockquote>“make both calculations and graphs. Both sorts of output should be stuidied; each will contribute to understanding”</blockquote> <pre>import matplotlib.pyplot as plt<br />for i in [[&#39;X1&#39;,&#39;Y1&#39;], [&#39;X2&#39;,&#39;Y2&#39;], [&#39;X3&#39;,&#39;Y3&#39;], [&#39;X4&#39;,&#39;Y4&#39;]]:<br />    plt.scatter(data[i[0]], data[i[1]])<br />    plt.show()</pre> <h3>Datasaurus Dozen</h3> <p>A modern take on the Anscombe quartet is the <a href="https://dl.acm.org/doi/10.1145/3025453.3025912">Datasaurus Dozen</a>. Given a distribution of points — in this example, a shape that looks remarkably like a dinosaur, the proposed system is able to use machine learning techniques (simulated annealing) to identify other configurations of the data points such that the underlying statistical properties match those of the original dataset. As the name suggests, the original depiction of a dinosaur can be mapped to 11 other data representations whilst also preserving the underlying stastistical properties.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/600/0*DnD5ZWPzgUnWEUn2.gif"/></figure> <h3>Correlation does not imply Causation</h3> <p>As a final point for discussion in this section, it is important to recognise a golden rule when working with statistics, and that is <a href="https://en.wikipedia.org/wiki/Correlation_does_not_imply_causation">correlation does not imply causation</a>. Ice cream sales may increase when the weather is sunny, and likewise shark attacks may increase when the weather is sunny. However, shark attacks are not caused by ice cream sales (nor are ice cream sales caused by shark attacks). In this example, the hidden variable that both attributes rely on is sunny weather — although there are actually many other factors and neither case is caused by a single variable.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/612/0*7aYtHckE0WfcwR49.png"/></figure> <p>When we are exploring data science for cyber security, we want to make well informed decisions from the data. It is important to recognise that attributes observed in the SOC may or may not necessarily be caused by other correlated attributes in your workforce. Further research explores <a href="https://www.astesj.com/publications/ASTESJ_050349.pdf">causal modelling in cyber security</a> to determine how effective this can be.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/354/0*Z8lBQ7cj0W07AtC2.png"/></figure> <h3>Further reading</h3> <ul><li><a href="https://ieeexplore.ieee.org/document/6725337">T. Mahmood and U. Afzal, “Security Analytics: Big Data Analytics for cybersecurity: A review of trends, techniques and tools,” 2013 2nd National Conference on Information Assurance (NCIA), 2013, pp. 129–134, doi: 10.1109/NCIA.2013.6725337.</a></li><li><a href="https://link.springer.com/article/10.1007/s41060-018-0102-5">Weihs, C., Ickstadt, K. Data Science: the impact of statistics. Int J Data Sci Anal 6, 189–194 (2018). https://doi.org/10.1007/s41060-018-0102-5</a></li><li><a href="https://link.springer.com/article/10.1007/s10699-016-9489-4">Calude, C.S., Longo, G. The Deluge of Spurious Correlations in Big Data. Found Sci 22, 595–612 (2017). https://doi.org/10.1007/s10699-016-9489-4</a></li><li><a href="https://www.jpands.org/vol19no2/briggs.pdf">Briggs, W.M. Common Statistical Fallacies. Journal of American Physicians and Surgeons, Volume 19, Number 2 (2014).</a></li></ul> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=d0f0d944dcc0" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">Security Data Analytics – A Pipeline Process</title><link href="https://pa-legg.github.io/blog/2023/security-data-analytics-a-pipeline-process/" rel="alternate" type="text/html" title="Security Data Analytics – A Pipeline Process"/><published>2023-09-11T21:58:36+00:00</published><updated>2023-09-11T21:58:36+00:00</updated><id>https://pa-legg.github.io/blog/2023/security-data-analytics--a-pipeline-process</id><content type="html" xml:base="https://pa-legg.github.io/blog/2023/security-data-analytics-a-pipeline-process/"><![CDATA[<h2>Security Data Analytics — Session 2 — A Pipeline Process</h2> <p><strong><em>*** This material is session 2 from the previous Security Data Analytics and Visualisation course that I led at UWE until 2024. It is now shared for reference purposes. ***</em></strong></p> <p>What would a typical pipeline or workflow look like in a cyber security data analytics investigation? We can think of this as a 5-stage process:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/667/0*6fN6wgjhmHlW7umb.png"/></figure> <ul><li><strong>Collect: </strong>What data do we need to collect, and how will we collect it? This is fundamental as it requires us to carefully think about the problem that we hope to address. Typically, we require a *sensor* in order to collect data — much like if we want to collect data about the world around us we need to use some sensor or measurement instrument to do this effectively. Sensors could be software-based, such as a Python script that continual checks and records every new login on a particular server, or that logs all the current running processes on a workstation. IoT devices may enable us to sense something about the physical world (for example, temperature) and transmit this data to a centralised server. Humans are also powerful sensors, often overlooked where thinking about cyber security. For example, humans may sense observations about the workplace such as employee engagement or disgruntlement, that if captured effectively could help mitigate potential threats.</li><li><strong>Store: </strong>How will we store this data? Is it practical to store the volume of data required? Is there are specific format that will make this data more useable for our further analysis and investigation? Common file data formats such as Comma Separated Values (CSV) and Javascript Object Notation (JSON) are popular amongst the data science communities. For larger distributed data collection, databases may be a preferable approach, that may be better suited to manage multiple users either reading or writing data simulateneously. Databases are widely used in complex systems, and typically either hold structured or unstructured data. Sequel (SQL) is a traditional database language used for structured data, whilst NoSQL unstructured methods such as MongoDB have been adopted more recently, that effectively store a collection of JSON documents. Whichever approach is taken, you will need to consider how often is data being written into your data store, how often it is being read from your data store, and how many users will need to do this at any one time, to manage the scalability of how your data is stored.</li><li><strong>Combine / Transform:</strong> Once we have a way of storing our data, what do we want to do with it? Typically, we may be looking to gather data over time to observe whether there is some change in behaviour, or we may be looking to combine multiple data sources to help explain why a particular observation has occurred, such as a cyber security breach or some suspicious activity on our network. We may also want to transform the data in some manner to make it easier to explore and understand. <a href="https://en.wikipedia.org/wiki/Aggregate_function">Aggregation</a> is a widely used process for transforming data. To aggregate data simply means to reduce this data to some form of summary; a common example would be to take the mean value of the data collected over a given time period. For example, given a number of raw packets captured from a network, we may want to know the total number of packets observed every minute, or the average size of the packets received each minute. This provides a much clearer indication when comparing activity over time, rather than studying individual packets. This can also be described as <a href="https://en.wikipedia.org/wiki/Feature_extraction">feature extraction</a> since we are deriving a set of features that characterise our raw data, which could then be used to inform a machine learning model. It may also be described as <a href="https://hackernoon.com/what-steps-should-one-take-while-doing-data-preprocessing-502c993e1caa">data pre-processing</a>, since we are essentially preparing our data so that it is in a more appropriate format to work with for further investigation. Other examples may include rescaling absolute values, such as rescaling RGB pixel values in an image or rescaling RAM usage to be a percentage of the total RAM available in a system.</li><li><strong>Analyse:</strong> We now have both the raw data and a feature-based representation of the data. We may sometimes refer to this as having <em>focus and context</em> of our data, where we can both drill down into the detail of each data point whilst observing something about the overall data distribution. With this in mind, we now need to establish what kind of analyse we need for our task. We may wish to examine data based on some thresholding techniques — for example, identify all cases where more than <em>X</em> network connections have been established during a 1-hour period. We may also wish to examine data using [<a href="https://ieeexplore.ieee.org/document/8716381">signature-based techniques</a> — for example, identify all software executables where the MD5 hash matches against a known set of examples (often described as a dictionary, and widely used for anti-virus detection). There are other more sophisticated forms of analysis we may also want to consider. We may want to perform <a href="https://towardsdatascience.com/machine-learning-classifiers-a5cc4e1b0623">classification</a> of our data, where we have a set of possible outcomes or groups, and we want to identify which group each instance of our data belongs to (for example, classification of different malware family samples). Another form of analysis that we may wish to perform is <a href="https://machinelearningmastery.com/findings-comparing-classical-and-machine-learning-methods-for-time-series-forecasting/">forecasting</a>. For example, if we have observed 2 data breaches in the last 12-month period, can we forecast whether we will fall victim again, based on possible mitigations that could be deployed. <a href="https://towardsdatascience.com/unsupervised-machine-learning-clustering-analysis-d40f2b34ae7e">Clustering</a> is another form that is widely used because it allows us to group similar observations together, to then examine whether they are similar under some set assumptions. This sounds similar to classification, however in classification we have labelled samples to train on, whereas in clustering we do not have any labels. We will discuss this further in Chapter 4. Finally, there is outlier and anomaly detection — a key part of security analysis. Given some observation, can we identify when something is different, and more specifically, under what conditions is it anomalous. We will describe this in further detail in due course.</li><li><strong>Visualise: </strong>Following our analysis, we need to plan how to communicate our findings. We may use 2-dimensional charts and plots to do this, such as line plots, bar charts, scatter plots, as well as other forms of visual plots. It’s important to appreciate that numerical values in a table also communicate data — sometimes a table of numerical values will perform more effectively than a visualisation, often if the data is small. As the data becomes larger however, visualisation helps to summarise this and convey details clearer. Depending on the task, we may find that 3-dimensional visualisation is appropriate. This could be a 3-dimensional plot, or it could be a 3-dimensional reconstruction of some scene, depending on the application. Importantly, we need to think about how the end-user will receive information from the chosen representation. Focus-and-context is a common technique to allow users to examine some aspect in detail, whilst also showing how this fits within the broader data. More and more nowadays, interactive techniques in visualisation need to be considered, for how a user will learn and understand the data through interaction, such as parameter adjustment, or zoom and filter of the data.</li></ul> <h4>Data Science Workflows</h4> <p>Whilst we have described a possible workflow above, it’s important to recognise that there is no single right way of doing data science. More specifically, if we keep in mind that we are doing analysis for a purpose, to help inform a story, whether that be exploratory or explanatory, we need to establish the question that we are asking, so that we can strive to answer this.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Mf_bK3I2Imc6F_cV.png"/></figure> <p>Here we see another possible workflow, where we decide on a question, and we acquire data to support this question. We clean and transform this data, and we perform our data analysis — much like we have discussed already. However, it is important to recognise that this is not a single one-time process, nor we will necessary ask the right question to being with. In this example, we see three “loops” back to earlier stages of the process — we may need to update our analysis, for example, if we decide that we need to consider more or different features about the data. We may examine our output and present our results, to find that we need more data — for example, if we are examining the presence of an insider threat, do we have sufficient information about the actions they have taken, or do we only have coverage of a subset (e.g., we may have network activity, but we have no record of file access activity). Finally, we may decide that we need to update our question, for example, is our original question not achievable, or do we need to be more specific with how the question is formed?</p> <p>Two further models of analysis are presented here — both showing how analysis is very much an iterative process that will continually evolve. (Further detail in the book by <a href="https://www.vismaster.eu/wp-content/uploads/2010/11/VisMaster-book-lowres.pdf">D. Keim “Mastering the Information Age: Solving Problems with Visual Analytics, 2010</a>). In the literature, often researchers will describe this as having a “human-in-the-loop” or by adopting a “visual analytics loop”. This is more and more common nowadays as dashboards and interactive analysis techniques have become a normal means of practice.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/720/0*yuKNsyvfe0Vq3AGZ.png"/></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/849/0*5oFmBRTyr0LlR0SC.png"/></figure> <h4>Introducing Python Notebooks</h4> <p>Notebooks are used for rapid prototyping of ideas, to test a theory or to derive some experiment. In particular, when we talk of notebooks we are describing computational notebooks, where we want to jot down ideas, and test these, whilst being able to jot down code samples and examine our results and findings quickly. Scripting languages such as Python and Javascript make it much easier to rapidly construct a piece of code to process some data, rather than having to compile code to source, like you would do in C or Java. Notebook environments provide a clean approach for integrating code, text, graphics, and other forms of media, using a web browser to interact with this rather than relying on traditional IDEs or text editors. Notebooks stem back to software tools such as Mathematica, however, the most commonly used Notebook format today is the Jupyter Notebook (formerly called the iPython notebook — hence the file extension ipynb). Jupyter is an amalgomation of Julia, Python, and R — the three programming languages it was originally designed for — however over the years a number of extensions have been developed to support many more languages, making it a defacto standard for computational notebooks. Notebooks essentially store their data in JSON format, and the Jupyter environment then renders this correctly for user interaction. More recently, Jupyter Lab has been proposed as a browser-based environment for working with notebooks that supports a number of modern features compared to the original Jupyter environment.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*rIPtmQ1ZBYHyNxIr.png"/></figure> <p>Python has grown in popularity significantly over the last 10 years. There are a number of reasons, such as the ability to use the REPL command line (Read-Evaluate-Print-Loop), which links well with our view of iterative practice for data science. Furthermore, the wealth of libraries developed for Python mean that many tasks can be performed quickly using these libraries rather than coding from scratch. In particular, for Data Science, there are a number of libraries that are seen as the standard: NumPy (Numerical Python), SciPy (Scientific Python), Matplotlib (a Python graphing library originating from MATLAB conventions), Pandas (Powerful Data Analysis), as well as scikit-learn (used for machine learning), Tensorflow, Keras and PyTorch (used for deep learning), and NLTK (Natural Language Toolkit).</p> <p>As part of this broader course, we will focus on a “Hello Security Data Analytics” challenge, to demonstrate how we can use a data science workflow with the Jupyter notebook environment to solve a cyber security challenge. The manager of the security operations centre (SOC) suggests that her analysts are becoming inundated with “trivial” alerts ever since a new data set of indicators was introduced into the Security Information and Event Management (SIEM) system. As a cyber security data scientist, they have asked for your help to reduce the number of “trivial” alerts without sacrificing visibility of security alerts. This is a good problem to tackle through data analysis, and we should be able to form a solid, practical question to ask after we perform some exploratory data analysis and hopefully arrive at an answer that can help the SOC. In the next session, we will look at how we can use Jupyter notebooks to help us solve this problem.</p> <h4>Further reading</h4> <ul><li><a href="https://jupyterlab.readthedocs.io/en/stable/">JupyterLab Documentation</a></li></ul> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=9be6356f0a8d" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">Security Data Analytics - An Introduction</title><link href="https://pa-legg.github.io/blog/2023/security-data-analytics-an-introduction/" rel="alternate" type="text/html" title="Security Data Analytics - An Introduction"/><published>2023-09-11T20:30:26+00:00</published><updated>2023-09-11T20:30:26+00:00</updated><id>https://pa-legg.github.io/blog/2023/security-data-analytics---an-introduction</id><content type="html" xml:base="https://pa-legg.github.io/blog/2023/security-data-analytics-an-introduction/"><![CDATA[<h3>Security Data Analytics — Session 1 — An Introduction</h3> <p><strong><em>*** This material is session 1 from the previous Security Data Analytics and Visualisation course that I led at UWE until 2024. It is now shared for reference purposes. ***</em></strong></p> <p>Data analytics is increasingly used as a first line of defence in cyber security – and yet many organisations are still to embrace this – partly due to a lack of resourcing and partly due to a lack of knowledge for what kind of security analysis they may even require. Fundamentally, we all live in a highly digital world, where we interact online, or we interact with other physical systems that are connected online, and where all these interactions result in more data being collected – from networking traffic to system and user logs – our computers generate massive volumes of data every minute. Now consider the different forms of digital equipment that are also connected and generating data – from laptops to smartphones, servers to light bulbs, payment card systems to CCTV, and anything else that may be Internet-enabled – we now have a wealth of data logs capturing a whole host of activities from these devices, that enable our technology-driven world.</p> <p>So, what happens when these devices do not perform as expected? What if our laptop suffers a malware attack, our CCTV is taken offline by a Denial of Service attack, or an employee downloads confidential records and copies these to a removable drive? Security data analytics is concerned with how we protect our systems and information from potential threats, through enhanced understanding of our systems and the data that underpins these.</p> <h4>What about Visualisation?</h4> <p>Analysing data is one thing – communicating this analysis to make well-informed decisions is another all together. Visualisation techniques enable us to make greater understanding of complex data, and ultimately allow us to make better decisions about our data.</p> <ul><li><strong>Exploratory visualisation</strong> is where visualisation methods may be used to explore a dataset to uncover some unknown traits or characteristics. It may not even be known what is being sought after initially, however through filtering and selection, the analyst may then discover new details within the data.</li><li><strong>Explanatory visualisation</strong> is used to communicate these findings to somebody else. A particular chart or graph may be used that emphasises some characteristic of the data so that this is clearly visible for others and so that it provides contextual meaning to explain the situation.</li></ul> <p>We can think about exploratory visualisation as supporting our analysis of data, and explanatory visualisation as supporting our presentation of data.</p> <blockquote><em>“The greatest value of a picture is when it forces us to notice what we never expected to see”</em> John W. Tukey, 1977</blockquote> <h4>What kind of data are we talking about?</h4> <p>Just as the security domain is broad, so to are the forms of data that may inform security. At the core is context – what are we trying to find out, and what data may help us to answer this question clearly? Furthermore, how do we then respond or act as a result of this information?</p> <p>We can think of data as the lowest level of a broader pyramid, where data informs information, information informs knowledge, and knowledge informs wisdom. Data in isolation may not be meaningful, however combined with other data, may help to build up towards these higher levels. This is often referred to as the <a href="https://en.wikipedia.org/wiki/DIKW_pyramid">DIKW pyramid</a>.</p> <h4><strong>So, what do we mean by cyber security data analytics?</strong></h4> <p>For an organisation to be secure, a clear understanding of the operational environment is required. This is often described as <a href="https://en.wikipedia.org/wiki/Situation_awareness">situational awareness</a>, which is the perception of environmental elements and events, the comprehension of their meaning, and the projection of their future states. Increasingly, organisations are deploying <strong>security operations centres (SOC)</strong>, where analysts will seek to identify suspicious behaviour and understand the context and relevance to the organisational mission, often using <strong>Security Information and Event Management (SIEM) </strong>systems.</p> <p>Technology underpins modern organisations and having insight into the business operational environment is crucial to protect it. As a first stage, ensuring the safe and correct operation of our computer systems, and our networking infrastructure is a good place to start. Network traffic data (e.g., packet captures) can help to indicate what data has been communicated over a network, and what actions have been carried out as a result of this (e.g., access to a particular URL, or downloading of large files). Intrusion Detection Systems (IDS) are commonly used to inspect networking inbound and outbound network traffic, to identify suspicious activities. IDSs will generate log files, and these logs constitute another informative data attribute. Similarly, firewall rules can help understand how the network is configured, and Intrusion Prevention Systems (IPS) will make decisions and act on IDS activity to prevent potential harm.</p> <p>The remit of cyber security is far and wide and goes beyond traditional computers and network security. A holistic view is required of what we want to protect, and what attack vectors may be used to gain access. Therefore, aspects such as physical security, people security, and process security also need to be understood. Physical security may require CCTV, IoT sensor monitoring or GPS tracking. People security may require text analytics of social media and email usage. Process security may require analysis of business process models, supply chain security, organisational hierarchy information, and operational practice. Technology continues to influence how we conduct business across the global, and therefore we need to ensure that we understand our threat landscape and have clear monitoring in place to understand potential harms. In many cases, we are interested in spatial-temporal data, i.e., in what location did the activity occur and at what time? Given our highly connected society, location is becoming increasingly challenging (are we looking at the location of the attacker, the location of the data, the location of the breached system?), and as for time, devices are logging activities faster than we can humanly inspect them. There is then the need for big data cyber security analytics – to make this flow of data manageable and insightful, to highlight key attributes in the data, and to enable informed decisions to be made to respond and react to potential threats.</p> <blockquote><strong>Security is about understanding systems, the people, and the processes that act upon these systems, such that they remain secure.</strong></blockquote> <p>Can we ever be fully secure? Probably not, but with greater insight of observed activities, we can manage this more effectively. Data analytics and visualisation techniques are one step towards achieving this.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/672/0*9i_pd-L96ixxE2V1.jpeg"/></figure> <h4>Data-driven Storytelling</h4> <p>We discussed earlier the idea that data visualisation can be used for exploratory and explanatory uses. In the latter, we may want to tell a story about our data, often described as Data-Driven Storytelling. So what makes for a good data story? There are five aspects that may be pertinent to the story.</p> <ul><li><strong>Trend</strong>: We may want to observe the historical or seasonal pattern of observations within our data, such as when do most people log on during the day, or what the monthly variation of demand on our servers is.</li><li><strong>Novelty</strong>: We are looking to find something new within our data observation, such as a new trend in our data.</li><li><strong>Outlier</strong>: We are looking to find where something occurs differently within an existing trend in our data.</li><li><strong>Forecasting</strong>: We want to determine what the future forecast of our data may be based on our historical observations.</li><li><strong>Debunking</strong>: We want to determine how our data supports or disproves a possible theory of what is happening.</li></ul> <p>Typically, in security, we are looking for novelty and outliers, based on the historical trend, to then provide a forecast of what may be if we do not intervene. Where we can couple observations against either “known-bad” activities (e.g., malware attacks), or show that observations are clearly deviating from “known-good” activities (e.g., insider threat), we can provide some insight into the underlying activity to determine whether action is required.</p> <h4><strong>AI and Cyber Security</strong></h4> <p>DarkTrace, CheckPoint, Symantec, Sophos, FireEye, Cynet, Fortinet, Vectra, and Cylance. These are just a handful of vendors that now use Artificial Intelligence and Machine Learning as a part of their products and services for cyber security defence – there are plenty others too, but this just gives you <a href="https://www.comparitech.com/blog/information-security/leading-ai-cybersecurity-companies/">an impression of the direction that the industry is moving in</a>.</p> <p>Cyber security requires a holistic view to identify what should be protected, and how may it be vulnerable to attack. The volume of data generated by today’s systems means that humans cannot analyse this raw data effectively. With AI and machine learning techniques, we can filter and manage data observations, whilst visualisation can help human analysts understand and communicate about observations and appropriate responsive actions.</p> <h4><strong>Further reading and resources</strong></h4> <p>Sarker, I.H., Kayes, A.S.M., Badsha, S. et al. <a href="https://journalofbigdata.springeropen.com/articles/10.1186/s40537-020-00318-5">Cybersecurity data science: an overview from machine learning perspective</a>. J Big Data 7, 41 (2020). https://doi.org/10.1186/s40537-020-00318-5</p> <p>Maayan, G. <a href="https://datasciencedojo.com/blog/data-science-cybersecurity">Cybersecurity revolutionized with rich data science</a>. Datasciencedojo (2020).</p> <p>BARC GmbH. <a href="https://www.youtube.com/watch?v=x1B9WrRPtOc">Big Data Security Analytics — Key Findings of the Study — YouTube</a> (2017).</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=2e9b524c772b" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry></feed>